<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rahul Nair</title>
    <link>https://rahulnair23.github.io/</link>
      <atom:link href="https://rahulnair23.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Rahul Nair</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 26 Oct 2021 14:14:05 +0100</lastBuildDate>
    <image>
      <url>https://rahulnair23.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Rahul Nair</title>
      <link>https://rahulnair23.github.io/</link>
    </image>
    
    <item>
      <title>Prospective Explanations</title>
      <link>https://rahulnair23.github.io/post/prospective-explanations/</link>
      <pubDate>Tue, 26 Oct 2021 14:14:05 +0100</pubDate>
      <guid>https://rahulnair23.github.io/post/prospective-explanations/</guid>
      <description>&lt;p&gt;At NeurIPS 2021 this year, Pierpaolo Tomassi and I have a demonstrator that brings together a few ideas on surrogates, bayesian networks, and model understanding. The 
&lt;a href=&#34;https://prosp-exp.eu-gb.mybluemix.net/ui/explorer/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;system&lt;/a&gt; (while its still up!) shows one view on &lt;em&gt;prospective&lt;/em&gt; explanations, one when requires some effort from users to build a mental model of ML model by probing interactively.&lt;/p&gt;
&lt;p&gt;The system is designed for regression and classification tasks with structured data. Prospective explanations are aimed at showing how models work by highlighting likely changes in model outcomes under changes in input (
&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3419764&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shneiderman 2020&lt;/a&gt;). This in contrast to most post-hoc explanability methods, that aim to provide a justification for a decision retrospectively. A desired property of explanations is to help with creating the right mental model of an AI system. The right mental model leads to greater trust (
&lt;a href=&#34;https://doi.org/10.1609/aaai.v33i01.33012429&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bansal et al., 2019&lt;/a&gt;). Interactive systems that allow for exploration have been shown to improve user comprehension (
&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3290605.3300789&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chen et al., 2019&lt;/a&gt;) albeit being more time consuming.&lt;/p&gt;
&lt;p&gt;Our system is designed to provide fast estimates of changes in outcomes for any arbitrary exploratory query from users. Such queries are typical partial, i.e. involve only a selected number of features, the outcomes labels are shown therefore as likelihoods. Repeated queries can therefore indicate which aspects of the feature space are more likely to influence the target variable.&lt;/p&gt;
&lt;p&gt;To achieve fast interactive exploration, we build a surrogate Bayesian network model. A surrogate here implies the use of model labels instead of ground truth labels to represent model behaviour. Bayesian network models are stored as directed acyclic graphs where links represent dependence between variables. This graph representation can be learnt directly from the data through a structure learning task, or be provided externally and only conditional probabilities estimated from the data. Bayesian networks are efficient in storing the joint distribution over feature sets and allow for fast inference over arbitrary queries.&lt;/p&gt;
&lt;p&gt;Specifically, we seek to explain a model $f$ that maps an input vector $\mathbf{x}_i$ to an output $y_i$. Consider a validation dataset $D$, a set of $(\mathbf{x}_i, y_i)$ observations for $i=1,&amp;hellip;,n$. For this validation data, we generate a set of labels $\hat{y}_i = f(\mathbf{x}_i)$. A structure learning algorithm is used, treating both features $\mathbf{x}_i$ and labels $\hat{y}_i$ as random variables to learn a graph $G(V, E)$ and associated probability tables. During inference, users provide arbitrary feature values, and the marginal distribution of the target class estimated using the formula&lt;/p&gt;
&lt;p&gt;$$P(x_1 = a_1, x_2 = a_2, &amp;hellip;, x_k=a_k,y=y_1) = \prod_{v=1}^{n} P(x_v=a_v| x_j = a_j , j\in pa(v))$$&lt;/p&gt;
&lt;p&gt;where $pa(v)$ denotes all parents of a node $v$ in $G$. While inference in Bayesian networks is NP-Hard, assumptions on the structure of $G$ admits fast inference in practice. We experimented with four strategies in particular. First, the network structure was defined &lt;em&gt;a priori&lt;/em&gt; from expert opinion leading to a lower node degree. Second, we experimented with limiting the number of parents for each node. Third, we learnt the structure on a limited set of important features as determined by feature important scores. Lastly, the network is training on a subset of features used in $f$. In our experiments, these approaches did not degrade performance of the surrogate model substantively.&lt;/p&gt;
&lt;p&gt;The main advantages of our approach are that (a) inference in most practical cases is very fast and supports real-time feedback allowing for interactivity, (b) inference can be done with partial information on features, and (c) any indirect effects are also considered in estimating target class distributions. Regression models involve an additional consideration. The target variable $\hat{y}_i$ is discretized before learning the structure. This is necessary to avoid the difficulty of perceiving changes in continuous probability distributions.&lt;/p&gt;
&lt;p&gt;The system provides two views. The first, a feature board showing all relevant features with clickable values that formulate queries. These are clustered by theme. Any selection results in updates to marginal distributions for all other variables.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;table.png&#34; alt=&#34;Feature board&#34;&gt;&lt;/p&gt;
&lt;p&gt;The second view is the learnt Bayesian network itself.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;graph.png&#34; alt=&#34;Network&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have deployed the system for several health and social care domains. Here we demonstrate it for a humanitarian application that deals with forced displacement. We explain two model classes for two tasks, one classification and one regression. Forced displacement refers to the involuntary movement of people away from their homes. This sector is highly complex, and a wide range of factors can potentially influence the onset and severity of displacement crisis. Responding organizations have to plan under high uncertainty on the type of humanitarian need. Previous user-studies for this sector has uncovered the need to surface this uncertainty (
&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3334480.3382903&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andres et al., 2020&lt;/a&gt;) which is both aleatory (uncertainty due to things that cannot be predicted) and epistemic (uncertainty due to missing information).&lt;/p&gt;
&lt;p&gt;While the regression task is to forecast volume of displaced persons, the classification task seeks to classify if the volume is likely to exceed a &amp;ldquo;crisis&amp;rdquo; threshold (the UNHCR informally deems a crisis when displacement volumes are in excess of 75,000 persons). For each task, prospective explanations of two models each that are training on data from a period from 1980 through 2015. Data after 2015 is used for the validation set $D$. For classification, a logistic regression with validation accuracy (acc) 97.2% and a gradient boosted classifier (acc: 98.03%) are shown. Both models exhibit poor precision for the minority class. Compared to ground truth reference, the models are less likely to predict a crisis. For regression we show a linear regression (MAE: 2086.03) and a gradient boosted regressor (MAE: 1155.69). For the latter, baseline performance (overestimates for 100-5000 ranges, and underestimates for $&amp;gt;5000$) can be seen. When uses toggle the human rights dimension to `worse&amp;rsquo;, predictions for &amp;gt;5000 better match ground truth labels. While this estimator is highly non-linear,  the impact of influencing factors can be understood by interacting with features values and visual representation of outcome marginals.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Partitioning a set</title>
      <link>https://rahulnair23.github.io/post/set-partition/</link>
      <pubDate>Fri, 30 Apr 2021 13:26:35 +0100</pubDate>
      <guid>https://rahulnair23.github.io/post/set-partition/</guid>
      <description>&lt;p&gt;In this post, we summarize the set partitioning problem, a widely applicable formalization that can be used in a variety of contexts. You are given a set of elements that need to be divided. Each division, or partition, has an associated cost that you seek to minimize. The set partitioning problem asks: what is the best way to partition such that the total cost can be minimized?&lt;/p&gt;
&lt;p&gt;This abstraction applied to a broad range of cases. From cutting cake for your family (if you consider cost to decrease as you get a larger piece) to more industrial applications like scheduling and crew rostering. There is also a large literature on the problem. A 
&lt;a href=&#34;https://doi.org/10.1137/1018115&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;survey&lt;/a&gt; paper was published way back in 1976 and reference on 
&lt;a href=&#34;https://doi.org/10.1007/978-1-4613-0303-9_9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;applications&lt;/a&gt; in 1998.&lt;/p&gt;
&lt;h2 id=&#34;the-problem&#34;&gt;The problem&lt;/h2&gt;
&lt;p&gt;In the basic setting, we have a set $X$ of elements and a collection of possible partitions $K={k_1, k_2,&amp;hellip;,k_m}$ and a cost function $c(k_j)$ associated with each partition. $K$ is a very large collection as there are several ways to divide a set. We define a membership matrix $A = [a_{ij}]$ where $a_{ij}=1$ if element $i$ from $X$ is in the partition $k_j$ and $0$ otherwise. We denote $x_j$ as a decision variable to decide if partition $k_j$ is employed/used or not. With these definitions, the set partitioning problem can be written as:&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
\min_{x} \quad &amp;amp; \sum_{j=1}^{n} c(k_j) x_j\\&lt;br&gt;
\textrm{s.t.} \quad &amp;amp; \sum_{j=1}^{m} a_{ij}x_j=1 \quad \forall i=1,&amp;hellip;,n\\&lt;br&gt;
&amp;amp;x_j\in {0,1}    \\&lt;br&gt;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The program seeks to find a set of partitions that divide the input feature space such that every element is covered by exactly one partition. (If we allow for overlap between partitions, we could use a coverage type constraint $\sum a_{ij}x_j \ge 1$ as well). The objective is to find a partitioning that minimizes the total cost of the partition.&lt;/p&gt;
&lt;h2 id=&#34;column-generation&#34;&gt;Column generation&lt;/h2&gt;
&lt;p&gt;The model above can not typically be solved directly for realistic sized instances. This is because the collection $K$ is very large. For cases where the number of columns is much greater than the rows, i.e. $m\gg n$, we can start with a restricted set of partitions and generate partitions on the fly as needed. The method of column generation is useful here to do this in a principled manner and can work as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Restricted master problem&lt;/strong&gt;: We start with a limited set of partitions $J, J\subset K$ and relax integral conditions to solve a linear program. Denote $\pi_i$ as the dual variables associated with each equality constraint. We solve this limited problem and recover the values of $\pi_i, i=1, &amp;hellip;, n$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generate new partitions&lt;/strong&gt;: Formulate a pricing problem that seeks to find partition that can have the largest benefit, in terms of the real objective.  The general approach is to find a column (i.e. partition) that has the largest negative 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Reduced_cost&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reduced cost&lt;/a&gt;. If $z_i$ as a binary decision variable that denotes if element $i$ is in the new partition, the problem of finding a new partition can be stated as&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\begin{aligned}
\min_{z} \quad &amp;amp; c(z) - \sum_{i=1}^{n} \pi_i z_i\\&lt;br&gt;
&amp;amp;z_i\in {0,1}    \\&lt;br&gt;
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where $c(z)$ denotes the cost of the chosen partition. Depending on your problem specifics and the nature of this function the manner in which you solve this would vary. Looking for the column with the greatest negative reduced cost is a heuristic step and can be replaced by other procedures to generate new candidate partitions if you have some insights that help you to do so.&lt;/p&gt;
&lt;h2 id=&#34;reference-implementation&#34;&gt;Reference implementation&lt;/h2&gt;
&lt;p&gt;To build some intuition, let&amp;rsquo;s look at a reference implementation for a trivial toy example. The objective is to find the optimal partition of the English alphabet. The cost of each partition is &lt;code&gt;1.0&lt;/code&gt; unit - so the optimal partition is trivially a set with all 26 letters. However, we start the program with smaller partitions to demonstrate that the column generation ends up with the optimal solution.&lt;/p&gt;
&lt;p&gt;Denote &lt;code&gt;X&lt;/code&gt; is the set of elements and &lt;code&gt;K&lt;/code&gt; is a initial set of partitions. These can be arbitrary subsets.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import string
X = list(string.ascii_lowercase)
K = [X[i:i + 5] for i in range(0, len(X), 5)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s start with the (restricted) master program which is formulated as a linear program.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def master(X: List, K: List[List]) -&amp;gt; cplex.Cplex:
    &amp;quot;&amp;quot;&amp;quot; Restricted master problem &amp;quot;&amp;quot;&amp;quot;

    prob = cplex.Cplex()
    numvar = len(K)

    names = list(map(func, K))
    var_type = [prob.variables.type.continuous] * numvar
    prob.variables.add(names=names,
                    lb=[0.0] * numvar,
                    ub=[1.0] * numvar,
                    types=var_type)
    prob.objective.set_sense(prob.objective.sense.minimize)
    prob.objective.set_linear([(n, cost(kj)) for n, kj in zip(names, K)])

    lhs = []
    for i in X:
        coeffs = [1.0 if i in kj  else 0.0 for kj in K]
        lhs.append(cplex.SparsePair(names, coeffs))
    prob.linear_constraints.add(lin_expr=lhs,
                                rhs=[1.0] * len(lhs),
                                senses=[&#39;E&#39;] * len(lhs),
                                names=X)
    prob.set_problem_type(cplex.Cplex.problem_type.LP)
    print(f&amp;quot;{len(lhs)} constraints and {len(names)} variables.&amp;quot;)

    return prob
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The column generation procedure will solve the master at every iteration. Retrieve the dual variable values and then use that in a pricing step to determine a new partition to add (more on this next). If the reduced cost is negative then the objective will improve by the addition. If not, then no new partition has been found to improve the solution and we can terminate.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;while True:
    p.solve()
    pi = [-p for p in p.solution.get_dual_values()]

    # Generate new partitions/columns
    K_new, rc_new = pricing(X, pi)

    # termination check
    if rc_new &amp;lt;= -eps:

        newvar = func(K_new)
        print(f&amp;quot;Iteration {i}: Adding &#39;{&#39;&#39;.join(K_new)}&#39; column with rc: {rc_new}.&amp;quot;)

        p.variables.add(names=[newvar],
                        lb=[0.0], ub=[1.0],
                        types=[p.variables.type.continuous])
        p.objective.set_linear(newvar, cost(K_new))
        for c in K_new:
            p.linear_constraints.set_coefficients(str(c), newvar, 1.0)

        p.set_problem_type(cplex.Cplex.problem_type.LP)
        i+=1
    else:
        print(&amp;quot;No improvement partition found - Terminating column generation.&amp;quot;)
        break
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now on to the pricing step - the step where new columns are generated. Here is where, in actual applications, the heavy lifting would happen. For this simple example, we can just generate partitions with negative duals and compute the reduced cost of this partition. In many cases, this can be a combinatorial problem (resulting in another integer program) or have some structure that can be leveraged (e.g. a shortest path or allow for dynamic programming solutions).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def pricing(X: List, pi: List, ncols: int=5) -&amp;gt; Tuple[List, float]:
    &amp;quot;&amp;quot;&amp;quot; Heuristic to generate new partitions &amp;quot;&amp;quot;&amp;quot;
    
    # set of elements that have negative dual variables
    K_new = [x for p, x in sorted(zip(pi, X), reverse=True) if p &amp;lt; 0.0]
    
    # reduced cost of new partition
    rc_new = rc(K_new, pi, X)

    return sorted(K_new), rc_new
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;putting-it-together&#34;&gt;Putting it together&lt;/h2&gt;
&lt;p&gt;Running this together as shown in this 
&lt;a href=&#34;https://gist.github.com/rahulnair23/b3be98553df6637f7b7fd4490e80991d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gist&lt;/a&gt; gives the following output:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;26 constraints and 3 variables.
Iteration 0: Adding &#39;aku&#39; column with rc: -2.0.
Iteration 1: Adding &#39;bku&#39; column with rc: -3.0.
Iteration 2: Adding &#39;ablu&#39; column with rc: -2.5.
Iteration 3: Adding &#39;abklv&#39; column with rc: -2.25.
Iteration 4: Adding &#39;ckluv&#39; column with rc: -2.2857142857142856.
Iteration 5: Adding &#39;abckmuv&#39; column with rc: -2.307692307692308.
Iteration 6: Adding &#39;abckluw&#39; column with rc: -2.736842105263158.
Iteration 7: Adding &#39;aclmvw&#39; column with rc: -6.777777777777778.
Iteration 8: Adding &#39;bdlmuvw&#39; column with rc: -3.2222222222222223.
Iteration 9: Adding &#39;cdkvw&#39; column with rc: -5.000000000000001.
Iteration 10: Adding &#39;acdklmuvx&#39; column with rc: -2.4818181818181815.
Iteration 11: Adding &#39;abdkmwx&#39; column with rc: -5.540540540540541.
Iteration 12: Adding &#39;bclmx&#39; column with rc: -8.879999999999999.
...
Iteration 174: Adding &#39;abcdefghijklmnopqrstuwxy&#39; column with rc: -0.10279605263157898.
Iteration 175: Adding &#39;abcdefghijklmnopqrstuvwxyz&#39; column with rc: -0.08656995788488508.
Optimal value: 1.0
Partition: &#39;abcdefghijklmnopqrstuvwxyz&#39; with cost: 1.0.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since each partition costs one unit, the optimal partition is one single partition.&lt;/p&gt;
&lt;p&gt;The number of possible partitions of a set is known as the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Bell_number&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bell number&lt;/a&gt;. For the 26 alphabet, the total number of possible partitions is very large (&lt;code&gt;49631246523618756274&lt;/code&gt; as per &lt;code&gt;sympy.bell&lt;/code&gt; in Python). In this example the procedure evaluated 175 partitions before arriving at the optimal one.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Zero-Suppressed Decision Diagrams and Independent Sets</title>
      <link>https://rahulnair23.github.io/post/decision-diagrams/</link>
      <pubDate>Tue, 26 Jan 2021 13:16:32 +0000</pubDate>
      <guid>https://rahulnair23.github.io/post/decision-diagrams/</guid>
      <description>&lt;p&gt;I stumbled across Binary Decision Diagrams (BDDs) by chance. They are an efficient data structure to represent sets of graphs. While a graph $G$ is a set of vertices $V$ along with a set of edges $E$ that connect the vertices, a graph set is a collection of subgraphs over the universe $V$. For example, a collection of feasible paths on $G$ would be a graph set. For a graph, if one has its corresponding BDD, specific types of queries can be handled with remarkable efficiency. See the 
&lt;a href=&#34;https://youtu.be/Q4gTV4r0zRs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; video from the creators of 
&lt;a href=&#34;https://github.com/takemaru/graphillion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;grahillion&lt;/a&gt; for an illustration of count queries. Knuth&amp;rsquo;s Art of Programming has an entire chapter devoted to this, but it hasn&amp;rsquo;t seen too many uses in the operations research community (at least as far as I&amp;rsquo;m aware). It seems like it could be pretty useful for several combinatorial optimization problems.&lt;/p&gt;
&lt;p&gt;So in this post, we&amp;rsquo;ll work through one use - that of finding maximal independent sets in a graph. An independent set is a subset of vertices $V$, no two of which are adjacent. A &lt;em&gt;maximal&lt;/em&gt; independent set is one where no additional nodes can be included and remain as an independent set. Not to be confused with the maxim&lt;strong&gt;um&lt;/strong&gt; set - which looks for an independent set with maximum weight.&lt;/p&gt;
&lt;p&gt;We follow the work described in 
&lt;a href=&#34;https://doi.org/10.1007/s10878-014-9722-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Morrison et al. (2014)&lt;/a&gt; on this problem which uses a Zero-suppressed Decision Diagram (ZDD), a variant of BDD where the false conditions are omitted. This is useful when the feasible solutions are sparse. There are several works that look into using ZDDs on graph and optimization problems, e.g. maximal cliques 
&lt;a href=&#34;https://doi.org/10.1109/EDTC.1997.582363&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Coudert, 1997)&lt;/a&gt; and 
&lt;a href=&#34;http://people.mpi-inf.mpg.de/alumni/d1/2019/behle/azove.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;0/1 enumeration&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;zdds&#34;&gt;ZDDs&lt;/h1&gt;
&lt;p&gt;In addition to a graph $G$, we also need a vertex ordering. While the ordering can be arbitrary, it does have an impact on the resulting ZDD. Each node in the ZDD, $Z_s$ is defined by a tuple $(v, lo, hi)$, where $v$ is the vertex it refers to in $G$ and $lo, hi$ are exactly two branches, known as the low and high branch, pointers to other nodes in the ZDD. These can be thought of as binary outcomes of the node from which they emanate. The terminal nodes in the ZDD have special meaning. The $\top$ refers to a &lt;strong&gt;true&lt;/strong&gt; outcome, while $\bot$ encodes a &lt;strong&gt;false&lt;/strong&gt; outcome.&lt;/p&gt;
&lt;p&gt;All this becomes clearer in the example, taken from Morrison&amp;rsquo;s paper, below. The 5-node graph on the left has a corresponding ZDD on the right that encodes all possible maximal independent sets. The solid arrows are high branches, while the dashed ones are low branches.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig.svg&#34; alt=&#34;Alt&#34;&gt;&lt;/p&gt;
&lt;p&gt;This Directed Acyclic Graph (DAG) encodes every possible maximal independent set. Every feasible path from the root node till the terminal node $\top$ represents a maximal independent set. The set it represented by nodes that have high branches coming out. For example, feasible path $v_1, v_2, v_3, v_4,\top$ represents an maximal independent set ${v_2, v_4}$ as these are the only nodes with solid (high) branches. There are several important properties of the ZDD that I skip here, like the uniqueness of nodes in $Z_s$, each node is encountered only once, etc.&lt;/p&gt;
&lt;p&gt;The ZDD can now be used to enumerate all maximal independent sets and support non-trivial operations that would be expensive otherwise, e.g. a count of all maximal independent sets without listing each one. There are other advantages as well. For instance, if there is a need to repeatedly query this ZDD, for example when node weights change, then one can compute this efficiently. Mostly, ZDD offer a compact representation of the underlying set.&lt;/p&gt;
&lt;h1 id=&#34;constructing-a-zdd&#34;&gt;Constructing a ZDD&lt;/h1&gt;
&lt;p&gt;To do all of this, the ZDD needs to be constructed first. There are few methods to do this for each class of problem you want to solve. 
&lt;a href=&#34;https://doi.org/10.1007/s10878-014-9722-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Morrison et al. (2014)&lt;/a&gt;, for example, tackle the case of the maximal independent set.&lt;/p&gt;
&lt;p&gt;In this case, they describe an algorithm that takes as input a graph and a vertex ordering (this ordering can be arbitrary - but does have an influence on the size of the ZDD). The procedure is recursive, and it each iteration a single node is considered. It first considers if a maximal set can be constructed from the remaining vertices. Remaining here are w.r.t. to the ordering. It also checks if the set is already maximal. For these two cases, the procedure returns with the terminal nodes, i.e. $\bot$ or $\top$. If these conditions are not met, it creates two branches for the current node based on the next undominated node. A new node is added if it hasn&amp;rsquo;t previously been created.&lt;/p&gt;
&lt;h1 id=&#34;reference-implementation&#34;&gt;Reference implementation&lt;/h1&gt;
&lt;p&gt;A reference implementation is forthcoming (when time permits!).&lt;/p&gt;
&lt;p&gt;There are existing packages like 
&lt;a href=&#34;https://github.com/takemaru/graphillion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;graphmillion&lt;/a&gt; that provide powerful abstractions for common problem classes. See also their accompanying 
&lt;a href=&#34;https://doi.org/10.1007/s10009-014-0352-z&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;. 
&lt;a href=&#34;http://people.mpi-inf.mpg.de/alumni/d1/2019/behle/azove.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Azove&lt;/a&gt; is another tool (uses binary decision diagrams) for vertex enumeration that may be of interest as well.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Artificial intelligence indocyanine green (ICG) perfusion for colorectal cancer intra-operative tissue classification</title>
      <link>https://rahulnair23.github.io/publication/cahill-2021-artificial/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/cahill-2021-artificial/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robust transit line planning based on demand estimates obtained from mobile phones</title>
      <link>https://rahulnair23.github.io/publication/lee-2021-robust/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/lee-2021-robust/</guid>
      <description></description>
    </item>
    
    <item>
      <title>User Driven Model Adjustment via Boolean Rule Explanations</title>
      <link>https://rahulnair23.github.io/publication/daly-2021-user/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/daly-2021-user/</guid>
      <description></description>
    </item>
    
    <item>
      <title>What Changed? Interpretable Model Comparison</title>
      <link>https://rahulnair23.github.io/publication/nair-2021-changed/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2021-changed/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Barriers to the CPLEX</title>
      <link>https://rahulnair23.github.io/post/cplex-barrier/</link>
      <pubDate>Tue, 27 Oct 2020 09:40:48 +0000</pubDate>
      <guid>https://rahulnair23.github.io/post/cplex-barrier/</guid>
      <description>&lt;h2 id=&#34;so-you-think-you-need-cplex&#34;&gt;So you think you need CPLEX?&lt;/h2&gt;
&lt;p&gt;To find out more, you review the 
&lt;a href=&#34;https://www.ibm.com/analytics/cplex-optimizer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;marketing material&lt;/a&gt;. After you wade past that you get to the technical documentation. The links lead you in loops. Where should you start? Is it the 
&lt;a href=&#34;http://ibmdecisionoptimization.github.io/docplex-doc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;decision optimization&lt;/a&gt;, 
&lt;a href=&#34;https://pypi.org/project/cplex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pip package&lt;/a&gt;, or is it 
&lt;a href=&#34;https://pypi.org/project/docplex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this pip package&lt;/a&gt;, or the 
&lt;a href=&#34;https://www.ibm.com/support/knowledgecenter/en/SSSA5P_12.8.0/ilog.odms.studio.help/Optimization_Studio/topics/COS_home.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;product documentation&lt;/a&gt;?&lt;/p&gt;
&lt;p&gt;After a while, you realize CPLEX has not just one Python API, but two. There is the the 
&lt;a href=&#34;https://www.ibm.com/support/knowledgecenter/SSSA5P_12.8.0/ilog.odms.cplex.help/CPLEX/Python/topics/PLUGINS_ROOT/ilog.odms.cplex.help/refpythoncplex/html/overview.html?view=kc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;legacy stuff&lt;/a&gt; and then the API that also allows for modeling called 
&lt;a href=&#34;https://cdn.rawgit.com/IBMDecisionOptimization/docplex-doc/master/docs/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DOcplex&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;pip install cplex&lt;/code&gt; seems to get you a solver - but only the &lt;em&gt;Community Edition&lt;/em&gt; that allows very small problems. If you need to solve larger problems or need more compute, you could use the 
&lt;a href=&#34;https://www.ibm.com/cloud/watson-studio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Watson Studio&lt;/a&gt; to run experiments on the cloud. Watson doesn&amp;rsquo;t call CPLEX CPLEX of course, but &lt;em&gt;Decision Optimization&lt;/em&gt;. To get the full version locally, you need the 
&lt;a href=&#34;https://www.ibm.com/ie-en/products/ilog-cplex-optimization-studio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CPLEX Optimization Studio&lt;/a&gt;. The docs also point to a dedicated cloud service, but this appears to have been 
&lt;a href=&#34;https://developer.ibm.com/docloud/try-docloud-free/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sunset&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;taps-and-streams&#34;&gt;Taps and streams&lt;/h2&gt;
&lt;p&gt;CPLEX has been around since 1988. I first used CPLEX through the academic initiative that IBM runs - granting unrestricted licenses for users at academic institutions. All you needed was a &lt;code&gt;.edu&lt;/code&gt; email address and you were good to go. It seems to have go through a winding road since.&lt;/p&gt;
&lt;p&gt;Software is only worth it if the problem they solve is repeatable. CPLEX certainly is in that bucket. Over time, mature software products devolve into a complex mesh of functionality, versions, and modes. You know a product has been around a while if you see a matrix! This is partly driven by the business which is fond of putting taps on a stream. Clients may have new uses that need new functions. And product teams have competition that drives some of this.&lt;/p&gt;
&lt;p&gt;In all this, the cloud increasingly seems like a barrier. Devs (and plebs) are spending considerable amounts of time to get at the technical realities, and once there, playing whack-a-mole for a reality that shifts over time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Oh Python</title>
      <link>https://rahulnair23.github.io/post/oh-python/</link>
      <pubDate>Thu, 20 Aug 2020 13:16:40 +0100</pubDate>
      <guid>https://rahulnair23.github.io/post/oh-python/</guid>
      <description>&lt;p&gt;Suppose you have a list of objects that you need to iterate over two consecutive items at a time.&lt;/p&gt;
&lt;p&gt;An old 
&lt;a href=&#34;https://stackoverflow.com/questions/16789776/iterating-over-two-values-of-a-list-at-a-time-in-python&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;stackoverflow&lt;/a&gt; question for this leads to the a quote from the 
&lt;a href=&#34;https://docs.python.org/2/library/functions.html#zip&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; that reads:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This makes possible an idiom for clustering a data series into n-length groups using &lt;code&gt;zip(*[iter(s)]*n)&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So the solution would be:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-(python)&#34;&gt;k = [1, 2, 3, 4, 5, 6]
list(zip(*[iter(k)]*2))
# [(1, 2), (3, 4), (5, 6)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That is cryptic! Let&amp;rsquo;s break it down to understand why this works.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Let&amp;rsquo;s start with the inner most bit. &lt;code&gt;iter&lt;/code&gt; simply returns an iterator object. For lists we would normally just write &lt;code&gt;for x in alist&lt;/code&gt; to iterate over the list, but under the hood an iterator is defined with each loop fetching the next item using a &lt;code&gt;next&lt;/code&gt; call.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-(python)&#34;&gt;&amp;gt;&amp;gt;&amp;gt; iter(k)
&amp;lt;list_iterator object at 0x7fcf654c9f28&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Next we consider &lt;code&gt;[iter(k)]*2&lt;/code&gt; - the multiplication here creates a shallow copy of the list.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-(python)&#34;&gt;&amp;gt;&amp;gt;&amp;gt; [iter(k)] * 2
[&amp;lt;list_iterator object at 0x7fcf654c9f28&amp;gt;, &amp;lt;list_iterator object at 0x7fcf654c9f28&amp;gt;]
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;The star operator &lt;code&gt;*&lt;/code&gt; then unpacks the collection as positional arguments to a function which is &lt;code&gt;zip&lt;/code&gt; in this case. &lt;code&gt;zip&lt;/code&gt; is a handy tool to merge several iterable together.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-(python)&#34;&gt;&amp;gt;&amp;gt;&amp;gt; zip(*[iter(k)] * 2)
&amp;lt;zip object at 0x7fcf654de808&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Finally, the &lt;code&gt;list&lt;/code&gt; operator just runs through to generate the entire list, giving us the desired output.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-(python)&#34;&gt;&amp;gt;&amp;gt;&amp;gt; list(zip(*[iter(k)] * 2))
[(1, 2), (3, 4), (5, 6)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What&amp;rsquo;s strange about all this is that it depends on subtle behaviours of the underlying methods. For example, instead of &lt;code&gt;zip(*[iter(k)] * 2)&lt;/code&gt; you wrote &lt;code&gt;list(zip(*[iter(k), iter(k)]))&lt;/code&gt;. You will end up with a different result. The solution depends on the iterators being a shallow copy! Each time any of the iterator is hit, it calls the &lt;code&gt;next&lt;/code&gt; call to the function.&lt;/p&gt;
&lt;h3 id=&#34;show-dont-tell&#34;&gt;Show, don&amp;rsquo;t tell&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;d hate to encounter snippets like this in the wild as it places significant cognitive load on people trying to read this. Strange it was included in the official 2.x documentation, thankfully removed from the current versions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Urban parks</title>
      <link>https://rahulnair23.github.io/post/on-parks/</link>
      <pubDate>Wed, 08 Jul 2020 20:53:17 +0100</pubDate>
      <guid>https://rahulnair23.github.io/post/on-parks/</guid>
      <description>&lt;p&gt;A 4.2 kilometre road cuts the 2000 acres of Phoenix Park neatly in half. Chesterfield Avenue doesn&amp;rsquo;t have a single pedestrian crossing, yield sign, or any amenity that isn&amp;rsquo;t designed for the car. During regular times, two lanes for the entire length serve as a parking lot. Throughout the park, pedestrians and cyclists have to stop and yield to passing cars. Its a strange hierarchy of movement to have within a park.&lt;/p&gt;
&lt;p&gt;A pandemic-induced change saw the Office of Public Works (OPW) designate two lanes as cycle-only and closure of peripheral gates to vehicles - both steps that brought some measure of traffic calm. The new minister, in opposition to his own department by all accounts, reversed one of these decisions and so its back being a thoroughfare.&lt;/p&gt;
&lt;p&gt;In all their public communication on why that is, a theme that stands out is their consideration for &amp;ldquo;An Garda Síochána and other key stakeholders&amp;rdquo;. In addition to being the largest walled park in Europe, the park is also home to several institutions. The residence of the Irish president and the U.S. Ambassador, the police headquarters, a hospital, a school, the zoo, cricket clubs, a polo ground, a visitor&amp;rsquo;s center, a nursing home, the Ordinance Survey of Ireland, several gatekeeper houses - some vacant, and a disused fort.&lt;/p&gt;
&lt;p&gt;So OPW, it would seem in public, is balancing institutional needs with those of the general public. All this takes me to another place and another time.&lt;/p&gt;
&lt;h2 id=&#34;the-peoples-park&#34;&gt;The People&amp;rsquo;s Park&lt;/h2&gt;
&lt;p&gt;Hot and humid air welcome those arriving at Chennai Terminal train station. As transit planners the world over have trouble connecting dots, the nearest suburban train station is a short trek away in Park Station. For a long time, it was a bit of mystery to me why it was called Park Station as all around it was concrete and asphalt.&lt;/p&gt;
&lt;iframe src=&#34;https://www.google.com/maps/embed?pb=!1m14!1m12!1m3!1d5254.7315661370585!2d80.27334477319604!3d13.0835764087889!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!5e0!3m2!1sen!2sie!4v1594282643148!5m2!1sen!2sie&#34; width=&#34;700&#34; height=&#34;350&#34; frameborder=&#34;0&#34; style=&#34;border:0;&#34; allowfullscreen=&#34;&#34; aria-hidden=&#34;false&#34; tabindex=&#34;0&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;That sector of Chennai was the centre of British colonial power who built Fort St. George around the corner. So not to different from Phoenix park really. It was smaller of course, a mere tenth of its Dublin cousin.&lt;/p&gt;
&lt;p&gt;What started in 1859 as a 
&lt;a href=&#34;https://en.wikipedia.org/wiki/People%27s_Park,_Chennai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;22 acre park&lt;/a&gt; with several ponds and green spaces, slowly morphed in the century to follow. It grew to be as large as 112 acres at one point. The city council took over the park in 1866. Several acres were carved out in 1886 for the Victoria Public Hall - built to commemorate Queen Victoria&amp;rsquo;s golden jubilee.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-the-victoria-public-hall-that-was-built-on-the-park-1888-sourcehttpscommonswikimediaorgwikifilevictoria_public_hall_chennaijpg&#34;&gt;



  &lt;img data-src=&#34;https://rahulnair23.github.io/post/on-parks/Victoria_Public_Hall,_Chennai_hue152d97b2aa3653a28b5e2e6b48ee7aa_3278914_2000x2000_fit_q90_lanczos.JPG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;400&#34; height=&#34;2304&#34;&gt;



  
  
  &lt;figcaption&gt;
    The Victoria Public Hall that was built on the park ~1888 &lt;a href=&#34;https://commons.wikimedia.org/wiki/File:Victoria_Public_Hall,_Chennai.JPG&#34;&gt;(source)&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Then there were stadiums and sports clubs followed by a 
&lt;a href=&#34;https://www.thehindu.com/news/cities/chennai/the-fire-that-changed-the-face-of-chennai-central/article7272827.ece&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;convenient fire&lt;/a&gt; in 1985 that was the final nail. The small part of the park that does remain is virtually inaccessible by the public and in 
&lt;a href=&#34;https://www.thehindu.com/todays-paper/tp-national/tp-tamilnadu/a-horticulturists-delight-a-picture-of-neglect-now/article27318169.ece&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;neglect&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So there you have it - a train station named after something that no longer exists. A cautionary tale on when government agencies serve each other rather than the general public. For Dublin, one can only hope that the 14 kilometre wall that keeps the deer in, keeps institutional needs out.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Model explanations via column generation</title>
      <link>https://rahulnair23.github.io/post/boolean-rules/</link>
      <pubDate>Tue, 16 Jun 2020 10:46:12 +0100</pubDate>
      <guid>https://rahulnair23.github.io/post/boolean-rules/</guid>
      <description>&lt;p&gt;In this post, I&amp;rsquo;ll review a paper from 2018 that deals with generating boolean decision rules and uses column generation. The 
&lt;a href=&#34;https://dl.acm.org/doi/10.5555/3327345.3327376&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; is well worth the read if you are interested in explainable AI models. The work also won the 
&lt;a href=&#34;https://community.fico.com/s/explainable-machine-learning-challenge&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FICO explainability challenge&lt;/a&gt; by applying this method to data from the financial services sector.&lt;/p&gt;
&lt;h1 id=&#34;setting&#34;&gt;Setting&lt;/h1&gt;
&lt;p&gt;Algorithmic decisions - ones where you rely on machines to reach a conclusion - require justification. Why was my loan approval denied? Why was the scan result classified as cancerous? In these and several other critical sectors, simply stating the prediction of a AI system is not enough. The underlying rationale of &amp;ldquo;why&amp;rdquo; is equally important.&lt;/p&gt;
&lt;p&gt;Several methods seek to come up with the justification algorithmically (in a field called Explainable AI) and several methods exist that can be distinguished along several dimensions. The main one is scope. The &lt;em&gt;global&lt;/em&gt; methods look to explain the entire model, i.e. how does the model behave? This contrasts with &lt;em&gt;local&lt;/em&gt; methods that look to explain a single instance, i.e. why was my loan not approved?&lt;/p&gt;
&lt;p&gt;When the models are complex, as is often the case, a popular class of methods are surrogate models. The basic goal here is to build a simpler (e.g. linear) version of a complex model, then look to interpret the simpler model in ways we humans can understand. When built for &lt;em&gt;local&lt;/em&gt; explanations, these probe the neighborhood of test instance to build a surrogate of the complex model.&lt;/p&gt;
&lt;p&gt;Unfortunately, it is hard to be objective when it comes to explanations. In practice, different methods will result in vastly different explanations for the same instance on the same underlying (complex) model. For minor changes in training samples or an adjustment of parameters, even the same method can give you very different results. Fundamentally, these approaches are limited by design. If you prescribe to the view that a machine learning (ML) model is all but a lossy compressed view on the data, then surrogate models are a lossy compressed view of the ML model. Significant challenges remain in practical deployments.&lt;/p&gt;
&lt;p&gt;An alternative view, the one described in the paper, is one where the model is directly interpretable. A directly interpretable model is one that can be understood by humans. There are of course, several models like decision trees that fall in this category.&lt;/p&gt;
&lt;h1 id=&#34;the-problem&#34;&gt;The problem&lt;/h1&gt;
&lt;p&gt;Consider a supervised binary classification task. You are given $(X_i, y_i)$ for observations $i = 1,&amp;hellip;, n$, where $X_i$ is the set of features associated with observation $i$ and $y_i$ is the binary outcome label. The task is to build a boolean classifier $\hat{y}(\mathbf{x})$ that can be stated as&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if (condition) then (predict True) else (predict False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;(condition)&lt;/code&gt; is of a specific form called a Disjunctive Normal Form (DNF). DNF clauses are OR of ANDs. A DNF clause on when to drink beer would be &lt;code&gt;(mood=HAPPY AND inventory&amp;gt;0) OR (mood=SAD AND inventory&amp;gt;0) OR (temp&amp;gt;15°C)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For a dataset there are exponentially many such clauses involving its features. The challenge is to find a relatively compact subset that provides the best prediction accuracy. The condition needs to be compact as overly complex clauses are not interpretable.&lt;/p&gt;
&lt;h1 id=&#34;the-model&#34;&gt;The model&lt;/h1&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://dl.acm.org/doi/10.5555/3327345.3327376&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; formulates the search for these clauses as a mixed-integer programming problem.&lt;/p&gt;
&lt;p&gt;$P$ denotes the set of positive samples, i.e. the observations where $y_i = 1$ and $Z$ denote negative ones. All features in $X_i$ are assumed to be binary valued. This isn&amp;rsquo;t too restrictive, continuous and categorical data can be encoded this way.  There is $K$ a set of (exponentially many) clauses involving features of $X$ and $K_i, K_i\subseteq K$ is the subset of clauses satisfied by observation $i$.&lt;/p&gt;
&lt;p&gt;There are two decision variables. First is $w_k$ (for all $k$ in set $K$) - a binary variable on if clause $k$ is selected for the model. Each clause $k$ in $K$ has an associated complexity $c_k$. The second is $\xi_i$ defined for $i \in P$ (i.e. for all positive samples) denotes all samples that are classified incorrectly.&lt;/p&gt;
&lt;p&gt;The objective looks to minimize Hamming loss which is the fraction of misclassified samples. Specifically, this can be written as&lt;/p&gt;
&lt;p&gt;$$
\min_{\xi, w} \color{blue}\underbrace{\color{black}\sum_{i\in P} \xi_i}_{\text{false negatives}} {\color{black}+}
\color{blue}\underbrace{\color{black}\sum_{i\in Z}\sum_{k\in K_i} w_k}_{\text{false positives}}
$$&lt;/p&gt;
&lt;p&gt;False positives add more than &amp;ldquo;one unit&amp;rdquo; if multiple clauses are satisfied. This is&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
\mbox{s.t.} \qquad &amp;amp; \xi_i + \sum_{k\in K_i} w_k \ge 1 \qquad \xi_i \ge 0, \qquad i\in P \\&lt;br&gt;
&amp;amp; \sum_{k\in K} c_k w_k \le C \\&lt;br&gt;
&amp;amp; w_k \in \{0, 1\} \qquad \forall k\in K
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The first constraint looks to identify false negatives. It basically says, for each positive sample, either chalk up a false negative ($\xi_i$) or include a rule that correctly represents this observation (i.e. a clause from the set $K_i$). The second constraint simply bounds the total complexity of the selected rule set to a parameter $C$.&lt;/p&gt;
&lt;p&gt;The overall problem still remains however, the set $K$ is very large and we&amp;rsquo;d like to avoid having to generate the entire set. This mainly because, it is expensive to generate and solve for all $K$. In any case, only a few $w_k$ will be selected in the final solution, so it makes sense to only look at clauses as needed.&lt;/p&gt;
&lt;p&gt;In the maximum clique 
&lt;a href=&#34;../max-clique&#34;&gt;post&lt;/a&gt; earlier, this type of exponential growth was addressed using constraint generation. In this work, they follow a column generation procedure - one of generating new variables ($w_k$) as needed. Adding rows and columns to a optimization program are strongly coupled topics. Adding a constraint (row) to a program is  the same as adding a variable (column) to its dual.&lt;/p&gt;
&lt;h2 id=&#34;the-pricing-problem&#34;&gt;The pricing problem&lt;/h2&gt;
&lt;p&gt;Now to the problem of generating new conjunctive clauses. The sub-problem looks to find the missing clause with the highest reduced cost, i.e. the clause that has the greatest impact on the objective function. This is a heuristic selection and you may have to undo a selection at a later step. But if no missing clause with a negative reduced cost can be found, the procedure terminates with the optimal solution.&lt;/p&gt;
&lt;p&gt;A bit more notation. From the program above, take $\mu_i$ to be the dual variables associated with the first constraint, and $\lambda$ be the dual variable for the complexity constraint. Define two sets of decision variables $\delta_i$ for all observations $i$ and $z_j$ if feature $j\in J$ is selected in missing constraint. Additionally denote $S_i$ to be a set of zero valued features for sample $i$.&lt;/p&gt;
&lt;p&gt;Armed with this, the sub-problem to identify a clause to include (with the greatest negative reduce cost) can be formulated as:&lt;/p&gt;
&lt;p&gt;$$
\min_{\delta, z} \color{blue}\underbrace{\color{black}\lambda\left(1+\sum_{j\in J}z_j\right)}_{\substack{\text{complexity of new rule}\\\text{in terms of features selected}}} {\color{black}-}
\color{blue}\underbrace{\color{black}\sum_{i\in P} \mu_i \delta_i}_{\substack{\text{how much the new rule}\\\text{improves false negatives}}}{\color{black}+}
\color{blue}\underbrace{\color{black}\sum_{i\in Z} \delta_i}_{\substack{\text{how much the new rule}\\\text{hurts false positives}}}
$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
\mbox{s.t.} \qquad &amp;amp; \delta_i + z_j\le 1 \qquad j\in S_i, \qquad i\in P \\&lt;br&gt;
&amp;amp;  \delta_i \ge 1- \sum_{j\in S_i} z_j \qquad \delta_i\ge 0 \qquad i\in Z\\&lt;br&gt;
&amp;amp; \sum_{j\in J} z_j \le D \\&lt;br&gt;
&amp;amp; z_j \in \{0, 1\} \qquad \forall j\in J
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The first constraint here says every zero-valued feature in every positive sample - the feature is either selected or the sample satisfies the clause. The next constraint states that the $i$-th negative sample is covered only if no zero-valued feature is selected. The next constraint bounds the complexity and the feature selection variables are restricted to be binary.&lt;/p&gt;
&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;The method appears to be well suited for transactional-type data, where there are some underlying rules (e.g. a business process) that generates the training data. I leave you with an interesting example. The tic-tac-toe 
&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Tic-Tac-Toe&amp;#43;Endgame&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dataset&lt;/a&gt; contains all possible board configurations at the end of a 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Tic-tac-toe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tic-tac-toe game&lt;/a&gt; and the outcome (if &lt;code&gt;x&lt;/code&gt; won or not). The method extracts almost all the rules of the game based on just this data, just missing the diagonal cases. Some of the rules extracted are superfluous.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-rules-extracted-from-tic-tac-toe-data&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://rahulnair23.github.io/post/boolean-rules/tictactoe_hu1ec8a325d060e5397d7307828d9e84ef_98406_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Rules extracted from tic-tac-toe data&#34;&gt;


  &lt;img data-src=&#34;https://rahulnair23.github.io/post/boolean-rules/tictactoe_hu1ec8a325d060e5397d7307828d9e84ef_98406_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1270&#34; height=&#34;618&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Rules extracted from tic-tac-toe data
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;As with all things, the approach comes with some limitations. There is a complexity-accuracy tradeoff, expect classifier accuracy to drop. Although in my experiments this was not substantial. The clauses generated are sensitive to the parameter $C$. If you don&amp;rsquo;t have a mechanism to validate the rule set then it is difficult to tune. The method doesn&amp;rsquo;t deal with class-imbalance, so you would need to under/over-sample to get a balanced sample. It would also worthwhile to study clauses generated in the presence of highly correlated features. I expect one of the correlated features to be picked arbitrarily.&lt;/p&gt;
&lt;h1 id=&#34;resources&#34;&gt;Resources&lt;/h1&gt;
&lt;p&gt;A modified implementation of the model is available through the 
&lt;a href=&#34;https://github.com/IBM/AIX360/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AIX360&lt;/a&gt; toolkit, so you can try it yourself. The differences are documented in another 
&lt;a href=&#34;https://arxiv.org/pdf/1909.03012.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;. Mainly, it uses a beam search heuristic instead of the pricing problem. Additionally, the complexity clauses are handled through two regularization terms, rather than a constraint.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Maximum weighted cliques in a graph</title>
      <link>https://rahulnair23.github.io/post/max-clique/</link>
      <pubDate>Fri, 05 Jun 2020 10:44:59 +0100</pubDate>
      <guid>https://rahulnair23.github.io/post/max-clique/</guid>
      <description>&lt;p&gt;Recently, I had the need to compute maximum weighted cliques on very dense large graphs. This is a well studied problem, and a nice 
&lt;a href=&#34;https://doi.org/10.1007/BF01098364&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;survey paper&lt;/a&gt; from 90&amp;rsquo;s by Pardalos and Xue gives a good overview of approaches.&lt;/p&gt;
&lt;h2 id=&#34;the-problem&#34;&gt;The problem&lt;/h2&gt;
&lt;p&gt;We are given a graph $G = (V, E)$ which is a set of vertices $V$ and edges $E$. Each vertex has an associated weight $d_i, \forall i\in V$. A &lt;em&gt;clique&lt;/em&gt; $C$ is a subset of vertices, where all vertices are pairwise connected. A &lt;em&gt;maximum&lt;/em&gt; clique is a clique that has largest weight.&lt;/p&gt;
&lt;p&gt;A related notion is of an &lt;em&gt;independent set&lt;/em&gt;, which is a subset of vertices $V$ that are pairwise disconnected. A maximum independent set is similarly an independent set with the largest weight.&lt;/p&gt;
&lt;h2 id=&#34;solutions&#34;&gt;Solutions&lt;/h2&gt;
&lt;p&gt;For general graphs, finding the maximum cliques is a hard problem (NP-complete). An integer programming approach that involves edges can be written as:&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
\max_x &amp;amp; \sum_{i\in V} d_i x_{i}\\&lt;br&gt;
\mbox{s.t.} \qquad &amp;amp; x_i + x_j \le 1 \quad \forall (i, j) \in \bar{E} \\&lt;br&gt;
&amp;amp; x_i \in {0, 1} \qquad \forall i\in V
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where $\bar{E}$ is called the complement edge set that is a set of edges that are missing from the original graph $G$. All the constraint $x_i + x_j \le 1$ says is if the edge $(i, j)$ is missing then only one node, either $i$ or $j$, can be in the clique. The objective seeks to maximize the weighted of selected nodes.&lt;/p&gt;
&lt;p&gt;As pointed out in the 
&lt;a href=&#34;https://doi.org/10.1007/BF01098364&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; this formulation isn&amp;rsquo;t useful in practice on account of two problems. First, the linear relaxation, one where the integrality constraints $x_i \in {0, 1}$ is omitted, gives a poor bound. The second is on account of symmetry. Symmetry arises in this context as vertices with the same weight are indistinguishable. So several configurations result in the same optimal solution. Why is this bad? The branch and cut tree cannot prune the search tree as solutions are in various parts of it. One way to remove symmetry is to do lexicographical ordering. An arbitrary order is imposed via additional constraints that cuts of several optimal solutions knowing that at least one optimal solution is valid. There are other methods as well, such as 
&lt;a href=&#34;https://doi.org/10.1007/s10107-002-0358-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;isomorphic pruning&lt;/a&gt;, and 
&lt;a href=&#34;https://doi.org/10.1016/j.disopt.2011.07.001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;orbitopal fixing&lt;/a&gt;, but we won&amp;rsquo;t get into those here.&lt;/p&gt;
&lt;h2 id=&#34;an-alternative-formulation&#34;&gt;An alternative formulation&lt;/h2&gt;
&lt;p&gt;The notion of the complement edge set $\bar{E}$ can be strengthened using independent sets. If you know an independent set, a clique can contain at most one vertex from such a set. To write this as a constraint, one would need to consider &lt;em&gt;maximum&lt;/em&gt; independent sets, lest you allow the omitted vertices to be included in the clique. Further, you would need to look at all maximum independent sets that arise in $G$. Assume for a moment that the set of maximum independent sets $\mathbb{S}$ is known. Then the problem of finding the maximum weighted clique can be written as&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
\max_x &amp;amp; \sum_{i\in V} d_i x_{i}\\&lt;br&gt;
\mbox{s.t.} \qquad &amp;amp; \sum_{i\in I} x_i \le 1 \quad \forall I \in \mathbb{S} \\&lt;br&gt;
&amp;amp; x_i \in {0, 1} \qquad \forall i\in V
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The objective is the same as before - maximize the total weight of selected nodes. The constraints, one for each maximum independent set, allows only one vertex into the solution. The constraints are tighter than the previous formulation implying that the linear relaxation gives a better bound. For some classes of graphs, namely perfect graphs, omitting the integrality constraints will directly give you integral solutions! The problem however is that there are now an exponential number of constraints (set $\mathbb{S}$ is very large).&lt;/p&gt;
&lt;p&gt;One mechanism to deal with too many constraints is use lazy constraints. The idea is to start the optimization with a small set of constraints and then add &lt;em&gt;relevant&lt;/em&gt; constraints from the large pool as you go along. The prerequisite is however that such relevant constraints can be generated efficiently.&lt;/p&gt;
&lt;p&gt;How would this work? A sketch of the solution algorithm looks like this.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Use a heuristic procedure to generate a set of maximum independent sets ($S$)&lt;/li&gt;
&lt;li&gt;Solve a linear relaxation on this limited set ($S \subseteq \mathbb{S}$)&lt;/li&gt;
&lt;li&gt;Based on solution, identify new maximum independent sets that would cut the current solution&lt;/li&gt;
&lt;li&gt;If no such sets exists, we are done (current solution gives the maximum weighted clique)&lt;/li&gt;
&lt;li&gt;If there are, then add to the constraints and goto step 2.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;reference-implementation&#34;&gt;Reference implementation&lt;/h2&gt;
&lt;p&gt;To test this, we use the &lt;code&gt;networkx&lt;/code&gt; library for graphs and the CPLEX solver. We use one of the many generators for a test graph with a known number of cliques.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from networkx import nx
G = nx.generators.ring_of_cliques(6, 3)
nx.draw(G, with_labels=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;gives you this:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-an-test-graph-with-known-max-cliques&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://rahulnair23.github.io/post/max-clique/graph_hu66fa372f34c55a7f200bf8e44ac04457_35825_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;An test graph with known max cliques&#34;&gt;


  &lt;img data-src=&#34;https://rahulnair23.github.io/post/max-clique/graph_hu66fa372f34c55a7f200bf8e44ac04457_35825_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    An test graph with known max cliques
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;step-1-generating-maximum-independent-sets&#34;&gt;(Step 1) Generating maximum independent sets&lt;/h4&gt;
&lt;p&gt;Here we use the greedy heuristic implementation as shown in this 
&lt;a href=&#34;https://kmutya.github.io/maxclique/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def greedy_init(G):
    &amp;quot;&amp;quot;&amp;quot;
    https://kmutya.github.io/maxclique/
    &amp;quot;&amp;quot;&amp;quot;
    n = G.number_of_nodes()  # Storing total number of nodes in &#39;n&#39;
    max_ind_sets = []  # initializing a list that will store maximum independent sets
    for j in G.nodes:
        R = G.copy()  # Storing a copy of the graph as a residual
        neigh = [n for n in R.neighbors(j)]  # Catch all the neighbours of j
        R.remove_node(j)  # removing the node we start from
        iset = [j]
        R.remove_nodes_from(neigh)  # Removing the neighbours of j
        if R.number_of_nodes() != 0:
            x = get_min_degree_vertex(R)
        while R.number_of_nodes() != 0:
            neigh2 = [m for m in R.neighbors(x)]
            R.remove_node(x)
            iset.append(x)
            R.remove_nodes_from(neigh2)
            if R.number_of_nodes() != 0:
                x = get_min_degree_vertex(R)

        max_ind_sets.append(iset)

    return(max_ind_sets)


def get_min_degree_vertex(Residual_graph):
    &#39;&#39;&#39;Takes in the residual graph R and returns the node with the lowest degree&#39;&#39;&#39;
    degrees = [val for (node, val) in Residual_graph.degree()]
    node = [node for (node, val) in Residual_graph.degree()]
    node_degree = dict(zip(node, degrees))
    return (min(node_degree, key=node_degree.get))
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;step-2-the-optimization-model&#34;&gt;(Step 2) The optimization model&lt;/h4&gt;
&lt;p&gt;Now we define the optimization model as a linear program using the CPLEX python API. We initialize the set of constraints based on the greedy heuristic.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cplex

prob = cplex.Cplex()
numvar = len(G.nodes)

def func(x): return &amp;quot;x&amp;quot;+str(x)

names = list(map(func, G.nodes))
var_type = [prob.variables.type.continuous] * numvar
prob.variables.add(names=names,
                   lb=[0.0] * numvar,
                   ub=[1.0] * numvar,
                   types=var_type)
prob.objective.set_sense(prob.objective.sense.maximize)
prob.objective.set_linear([(n, 1.0) for n in names])
lhs = []

# Call the greedy heuristic to generate a starting set of independent sets
mis = greedy_init(G)

for iset in mis:
    con_vars = [func(i) for i in iset]
    coeffs = [1.0] * len(con_vars)
    lhs.append(cplex.SparsePair(con_vars, coeffs))
prob.linear_constraints.add(lin_expr=lhs,
                            rhs=[1.0] * len(lhs),
                            senses=[&#39;L&#39;] * len(lhs))
print(&amp;quot;Constraint: Maximum independent set. ({} constraints)&amp;quot;.format(len(mis)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To solve this model, we would use the following snippet to execute the model and return the final solution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prob.solve()
status = prob.solution.status[prob.solution.get_status()]
print(&amp;quot;Status:{}&amp;quot;.format(status))

if prob.solution.get_status() in [101, 105, 107, 111, 113]:
    # Optimal/feasible, so get the solution
    print(&amp;quot;Solution value: &amp;quot;)
    print(prob.solution.get_objective_value())

    # get the configuration
    x_res = prob.solution.get_values(names)
    for x_name, val in zip(names, x_res):
        if val &amp;gt; 0:
            print(x_name, val)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;step-3-generate-lazy-constraints&#34;&gt;(Step 3) Generate lazy constraints&lt;/h4&gt;
&lt;p&gt;We&amp;rsquo;ve not managed the lazy constraints yet. To do this we will use a CPLEX &lt;code&gt;Callback&lt;/code&gt;. The &lt;code&gt;LazyConstraintCallback&lt;/code&gt; is called each time an optimal or integral solution is found. The implementation looks like this.&lt;/p&gt;
&lt;p&gt;To find new independent sets, we take the solution (potentially fractional) and use a greedy heuristic to first generate an independent set on the induced subgraph of the solution. We then expand on the independent set for the entire graph using another &lt;code&gt;greedy_expand&lt;/code&gt; procedure which uses the same logic as &lt;code&gt;greedy_init&lt;/code&gt; to grown the independent set.&lt;/p&gt;
&lt;p&gt;If there are no additional independent sets, no constraints are added and the solver terminates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class LazyCallback(LazyConstraintCallback):
    &amp;quot;&amp;quot;&amp;quot;Lazy constraint callback to generate maximum independent sets on the fly.

    There are too many such constraints to make them all available to 
    CPLEX right away - and in any case, very few of them are valid.

    So generate them on the fly.
    &amp;quot;&amp;quot;&amp;quot;

    # Callback constructor.
    #
    # Any needed fields are set externally after registering the callback.
    def __init__(self, env):
        super().__init__(env)

    def __call__(self):

        values = self.get_values(self.names)

        # Any node with non-zero value is considered as part of the set
        curr_solution = [int(name[1:]) for name, val in zip(self.names, values) if val &amp;gt;= 0.001]
        print(&amp;quot;Current solution: &amp;quot;, curr_solution)

        # Look to generate all independent sets that would cut off the (fractional)
        # value. To do this, first induce a subgraph - and for each node, built a
        #
        subG = self.G.subgraph(curr_solution)
        sub_ind_set = greedy_init(subG)
        max_ind_sets = [greedy_expand(self.G, sset) for sset in sub_ind_set]

        for iset in max_ind_sets:

            con_vars = [func(i) for i in iset]
            coeffs = [1.0] * len(con_vars)
            lhs = cplex.SparsePair(con_vars, coeffs)
            self.add(constraint=lhs, rhs=1.0, sense=&#39;L&#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The callback must be registered with the problem instance and any variables passed as attributes as so:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from cplex.callbacks import LazyConstraintCallback

# register callbacks to generate additional independent sets on the fly
lazycb = prob.register_callback(LazyCallback)

# pass any arguments as class attributes
lazycb.names = names
lazycb.G = G
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For completeness, here is what the &lt;code&gt;greedy_expand&lt;/code&gt; function does&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def greedy_expand(G, init_set):

    R = G.copy()
    neigh = [n for i in init_set for n in R.neighbors(i)]
    R.remove_nodes_from(init_set)
    R.remove_nodes_from(neigh)
    if R.number_of_nodes() != 0:
        x = get_min_degree_vertex(R)
    while R.number_of_nodes() != 0:
        neigh2 = [m for m in R.neighbors(x)]
        R.remove_node(x)
        init_set.append(x)
        R.remove_nodes_from(neigh2)
        if R.number_of_nodes() != 0:
            x = get_min_degree_vertex(R)

    return init_set
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;putting-it-all-together&#34;&gt;Putting it all together&lt;/h4&gt;
&lt;p&gt;Running this all together as shown in this 
&lt;a href=&#34;https://gist.github.com/rahulnair23/ef3c14a3f08afdf0840459e10969eda8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gist&lt;/a&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/rahulnair23/ef3c14a3f08afdf0840459e10969eda8.js&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;gives the following output:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Constraint: Maximum independent set. (18 constraints)
Version identifier: 12.10.0.0 | 2019-11-26 | 843d4de
CPXPARAM_Read_DataCheck                          1
Warning: Control callbacks may disable some MIP features.
Lazy constraint(s) or lazy constraint/branch callback is present.
    Disabling dual reductions (CPX_PARAM_REDUCE) in presolve.
    Disabling non-linear reductions (CPX_PARAM_PRELINEAR) in presolve.
    Disabling presolve reductions that prevent crushing forms.
         Disabling repeat represolve because of lazy constraint/incumbent callback.
Tried aggregator 1 time.
MIP Presolve eliminated 6 rows and 0 columns.
Reduced MIP has 12 rows, 18 columns, and 72 nonzeros.
Reduced MIP has 0 binaries, 0 generals, 0 SOSs, and 0 indicators.
Presolve time = 0.00 sec. (0.02 ticks)
MIP emphasis: balance optimality and feasibility.
MIP search method: traditional branch-and-cut.
Parallel mode: none, using 1 thread.
Root relaxation solution time = 0.00 sec. (0.01 ticks)
Current solution:  [5, 7, 8, 14, 16, 17]
Current solution:  [4, 10, 11, 13]
Current solution:  [0, 6, 8, 10, 11]
Current solution:  [4, 6, 8, 10, 11, 16]
Current solution:  [0, 2, 4, 5, 6, 8]
Current solution:  [6, 7, 8]
Current solution:  [6, 7, 8]

        Nodes                                         Cuts/
   Node  Left     Objective  IInf  Best Integer    Best Bound    ItCnt     Gap         Variable B NodeID Parent  Depth

*     0     0      integral     0        3.0000        6.0000        8  100.00%                        0             0
Elapsed time = 0.02 sec. (0.16 ticks, tree = 0.00 MB, solutions = 1)

User cuts applied:  17

Root node processing (before b&amp;amp;c):
  Real time             =    0.02 sec. (0.16 ticks)
Sequential b&amp;amp;c:
  Real time             =    0.00 sec. (0.00 ticks)
                          ------------
Total (root+branch&amp;amp;cut) =    0.02 sec. (0.16 ticks)
Status:MIP_optimal
Solution value: 
3.0
x6 1.0
x7 1.0
x8 1.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This identified one of the 3-vertex cliques, which is the maximum. The program started with 18 maximum independent sets generated greedily. It generated a further 17 user cuts one for each new independent set that were constructed on the fly. For a graph with $n$ nodes, there can be at most $3^{n/3}$ maximum independent sets although most have far fewer. For our 18 node graph, that would be 729 sets. In this greedy solution method, we got away with generating just 35.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Perfusion Quantification from Endoscopic Videos: Learning to Read Tumor Signatures</title>
      <link>https://rahulnair23.github.io/publication/zhuk-2020-perfusion/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/zhuk-2020-perfusion/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Scenario-based XAI for Humanitarian Aid Forecasting</title>
      <link>https://rahulnair23.github.io/publication/andres-2020-scenario/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/andres-2020-scenario/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Keep it simple stupid! A non-parametric kernel regression approach to forecast travel speeds</title>
      <link>https://rahulnair23.github.io/publication/nair-2020-keep/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2020-keep/</guid>
      <description></description>
    </item>
    
    <item>
      <title>RSM: An Explainable Predictive Sales Route Selector</title>
      <link>https://rahulnair23.github.io/publication/chen-2019-rsm/</link>
      <pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/chen-2019-rsm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A machine learning approach to scenario analysis and forecasting of mixed migration</title>
      <link>https://rahulnair23.github.io/publication/nair-2019-machine/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2019-machine/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An ensemble prediction model for train delays</title>
      <link>https://rahulnair23.github.io/publication/nair-2019-ensemble/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2019-ensemble/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Towards Automated Extraction of Business Constraints from Unstructured Regulatory Text</title>
      <link>https://rahulnair23.github.io/publication/nair-2018-towards/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2018-towards/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A predictive-control framework to address bus bunching</title>
      <link>https://rahulnair23.github.io/publication/andres-2017-predictive/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/andres-2017-predictive/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Spillover detection for urban traffic networks using signal timing and stop line detector data</title>
      <link>https://rahulnair23.github.io/publication/wang-2016-spillover/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/wang-2016-spillover/</guid>
      <description></description>
    </item>
    
    <item>
      <title> A multi-scale approach to data-driven mass migration analysis</title>
      <link>https://rahulnair23.github.io/publication/mohammed-2016-multiscale/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/mohammed-2016-multiscale/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Allaboard: visual exploration of cellphone mobility data to optimise public transport</title>
      <link>https://rahulnair23.github.io/publication/di-2016-allaboard/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/di-2016-allaboard/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Data-Driven Transit Network Design From Mobile Phone Trajectories</title>
      <link>https://rahulnair23.github.io/publication/pinelli-2016-data/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/pinelli-2016-data/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Docit: an integrated system for risk-averse multimodal journey advising</title>
      <link>https://rahulnair23.github.io/publication/botea-2016-docit/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/botea-2016-docit/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Characterization of network traffic processes under adaptive traffic control systems</title>
      <link>https://rahulnair23.github.io/publication/pascale-2015-isttt/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/pascale-2015-isttt/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Characterization of network traffic processes under adaptive traffic control systems</title>
      <link>https://rahulnair23.github.io/publication/pascale-2015-trc/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/pascale-2015-trc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Optimizing dial-a-ride services in Maryland: Benefits of computerized routing and scheduling</title>
      <link>https://rahulnair23.github.io/publication/markovic-2015-optimizing/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/markovic-2015-optimizing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Allaboard: visual exploration of cellphone mobility data to optimise public transport</title>
      <link>https://rahulnair23.github.io/publication/sbodio-2014-allaboard/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/sbodio-2014-allaboard/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Data as a resource: real-time predictive analytics for bus bunching</title>
      <link>https://rahulnair23.github.io/publication/nair-2015-data/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2015-data/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Equilibrium design of bicycle sharing systems: the case of Washington D.C.</title>
      <link>https://rahulnair23.github.io/publication/nair-2014-equ-dc/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2014-equ-dc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Equilibrium network design of shared-vehicle systems</title>
      <link>https://rahulnair23.github.io/publication/nair-2014-equilibrium/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2014-equilibrium/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semantic Traffic Diagnosis with STAR-CITY: Architecture and Lessons Learned from Deployment in Dublin, Bologna, Miami and Rio</title>
      <link>https://rahulnair23.github.io/publication/lecue-2014-semantic/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/lecue-2014-semantic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>AllAboard: a system for exploring urban mobility and optimizing public transport using cellphone data</title>
      <link>https://rahulnair23.github.io/publication/berlingerio-2013-allaboard/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/berlingerio-2013-allaboard/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Large-scale transit schedule coordination based on journey planner requests</title>
      <link>https://rahulnair23.github.io/publication/nair-2013-trb/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2013-trb/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robust Dynamic Distribution of Security Assets in Transit Systems</title>
      <link>https://rahulnair23.github.io/publication/nair-2013-robust/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2013-robust/</guid>
      <description></description>
    </item>
    
    <item>
      <title>2012 Maryland State Highway Mobility Report</title>
      <link>https://rahulnair23.github.io/publication/sh-amobilityreport/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/sh-amobilityreport/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Porous Flow Model for Disordered Heterogeneous Traffic Streams</title>
      <link>https://rahulnair23.github.io/publication/nair-2012-porous/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2012-porous/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Large-Scale Vehicle Sharing Systems: Analysis of Vélib&#39;</title>
      <link>https://rahulnair23.github.io/publication/nair-2012-large/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2012-large/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Missed connections: quantifying and optimizing multi-modal interconnectivity in cities</title>
      <link>https://rahulnair23.github.io/publication/coffey-2012-missed/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/coffey-2012-missed/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Relation between Real-Time Data and Hourly Traffic Volume Taking Heavy Vehicles into Consideration</title>
      <link>https://rahulnair23.github.io/publication/sadrsadat-2012-relation/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/sadrsadat-2012-relation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Geographic Information System-Based Real-Time Decision Support Framework for Routing Vehicles Carrying Hazardous Materials</title>
      <link>https://rahulnair23.github.io/publication/kim-2011-geographic/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/kim-2011-geographic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A porous flow approach to modeling heterogeneous traffic in disordered systems</title>
      <link>https://rahulnair23.github.io/publication/nair-2011-isttt/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2011-isttt/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A porous flow approach to modeling heterogeneous traffic in disordered systems</title>
      <link>https://rahulnair23.github.io/publication/nair-2011-porous/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2011-porous/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fleet management for vehicle sharing operations</title>
      <link>https://rahulnair23.github.io/publication/nair-2011-fleet/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2011-fleet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Design and Analysis of Vehicle Sharing Programs: A Systems Approach</title>
      <link>https://rahulnair23.github.io/publication/nair-2010-design/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2010-design/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Resilience Framework for Ports and Other Intermodal Components</title>
      <link>https://rahulnair23.github.io/publication/nair-2010-resilience/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2010-resilience/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Evaluation of relocation strategies for emergency medical service vehicles</title>
      <link>https://rahulnair23.github.io/publication/nair-2009-evaluation/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2009-evaluation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Security and Mobility of Intermodal Freight Networks</title>
      <link>https://rahulnair23.github.io/publication/miller-2009-security/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/miller-2009-security/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Application and Validation of Dynamic Freight Simulation-Assignment Model to Large-Scale Intermodal Rail Network: Pan-European Case</title>
      <link>https://rahulnair23.github.io/publication/zhang-2008-application/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/zhang-2008-application/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamic Relocation of Scarce Resources: The Case of Emergency Medical Service Vehicles in Montreal</title>
      <link>https://rahulnair23.github.io/publication/nair-2008-dynamic/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2008-dynamic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Market potential for international rail-based intermodal services in Europe: From sea to shining sea</title>
      <link>https://rahulnair23.github.io/publication/nair-2008-market/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2008-market/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Assessing Service Design Options and Strategies for Overcoming Barriers in the Reorient Intermodal Freight Transport Corridor</title>
      <link>https://rahulnair23.github.io/publication/miller-2007-assessing/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/miller-2007-assessing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modelling corridor freight transport for demonstration of seamless international rail freight services</title>
      <link>https://rahulnair23.github.io/publication/arcot-2007-modelling/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/arcot-2007-modelling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Application of floating car data using GIS for dynamic vehicular routing: A case study</title>
      <link>https://rahulnair23.github.io/publication/nair-2004/</link>
      <pubDate>Thu, 01 Jan 2004 00:00:00 +0000</pubDate>
      <guid>https://rahulnair23.github.io/publication/nair-2004/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
