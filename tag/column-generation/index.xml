<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>column generation | Rahul Nair</title>
    <link>https://rahulnair23.github.io/tag/column-generation/</link>
      <atom:link href="https://rahulnair23.github.io/tag/column-generation/index.xml" rel="self" type="application/rss+xml" />
    <description>column generation</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 30 Apr 2021 13:26:35 +0100</lastBuildDate>
    <image>
      <url>https://rahulnair23.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>column generation</title>
      <link>https://rahulnair23.github.io/tag/column-generation/</link>
    </image>
    
    <item>
      <title>Partitioning a set</title>
      <link>https://rahulnair23.github.io/post/set-partition/</link>
      <pubDate>Fri, 30 Apr 2021 13:26:35 +0100</pubDate>
      <guid>https://rahulnair23.github.io/post/set-partition/</guid>
      <description>&lt;p&gt;In this post, we summarize the set partitioning problem, a widely applicable formalization that can be used in a variety of contexts. You are given a set of elements that need to be divided. Each division, or partition, has an associated cost that you seek to minimize. The set partitioning problem asks: what is the best way to partition such that the total cost can be minimized?&lt;/p&gt;
&lt;p&gt;This abstraction applied to a broad range of cases. From cutting cake for your family (if you consider cost to decrease as you get a larger piece) to more industrial applications like scheduling and crew rostering. There is also a large literature on the problem. A 
&lt;a href=&#34;https://doi.org/10.1137/1018115&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;survey&lt;/a&gt; paper was published way back in 1976 and reference on 
&lt;a href=&#34;https://doi.org/10.1007/978-1-4613-0303-9_9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;applications&lt;/a&gt; in 1998.&lt;/p&gt;
&lt;h2 id=&#34;the-problem&#34;&gt;The problem&lt;/h2&gt;
&lt;p&gt;In the basic setting, we have a set $X$ of elements and a collection of possible partitions $K={k_1, k_2,&amp;hellip;,k_m}$ and a cost function $c(k_j)$ associated with each partition. $K$ is a very large collection as there are several ways to divide a set. We define a membership matrix $A = [a_{ij}]$ where $a_{ij}=1$ if element $i$ from $X$ is in the partition $k_j$ and $0$ otherwise. We denote $x_j$ as a decision variable to decide if partition $k_j$ is employed/used or not. With these definitions, the set partitioning problem can be written as:&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
\min_{x} \quad &amp;amp; \sum_{j=1}^{n} c(k_j) x_j\\
\textrm{s.t.} \quad &amp;amp; \sum_{j=1}^{m} a_{ij}x_j=1 \quad \forall i=1,&amp;hellip;,n\\
&amp;amp;x_j\in {0,1}    \\
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The program seeks to find a set of partitions that divide the input feature space such that every element is covered by exactly one partition. (If we allow for overlap between partitions, we could use a coverage type constraint $\sum a_{ij}x_j \ge 1$ as well). The objective is to find a partitioning that minimizes the total cost of the partition.&lt;/p&gt;
&lt;h2 id=&#34;column-generation&#34;&gt;Column generation&lt;/h2&gt;
&lt;p&gt;The model above can not typically be solved directly for realistic sized instances. This is because the collection $K$ is very large. For cases where the number of columns is much greater than the rows, i.e. $m\gg n$, we can start with a restricted set of partitions and generate partitions on the fly as needed. The method of column generation is useful here to do this in a principled manner and can work as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Restricted master problem&lt;/strong&gt;: We start with a limited set of partitions $J, J\subset K$ and relax integral conditions to solve a linear program. Denote $\pi_i$ as the dual variables associated with each equality constraint. We solve this limited problem and recover the values of $\pi_i, i=1, &amp;hellip;, n$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generate new partitions&lt;/strong&gt;: Formulate a pricing problem that seeks to find partition that can have the largest benefit, in terms of the real objective.  The general approach is to find a column (i.e. partition) that has the largest negative 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Reduced_cost&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reduced cost&lt;/a&gt;. If $z_i$ as a binary decision variable that denotes if element $i$ is in the new partition, the problem of finding a new partition can be stated as&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\begin{aligned}
\min_{z} \quad &amp;amp; c(z) - \sum_{i=1}^{n} \pi_i z_i\\
&amp;amp;z_i\in {0,1}    \\
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where $c(z)$ denotes the cost of the chosen partition. Depending on your problem specifics and the nature of this function the manner in which you solve this would vary. Looking for the column with the greatest negative reduced cost is a heuristic step and can be replaced by other procedures to generate new candidate partitions if you have some insights that help you to do so.&lt;/p&gt;
&lt;h2 id=&#34;reference-implementation&#34;&gt;Reference implementation&lt;/h2&gt;
&lt;p&gt;To build some intuition, let&amp;rsquo;s look at a reference implementation for a trivial toy example. The objective is to find the optimal partition of the English alphabet. The cost of each partition is &lt;code&gt;1.0&lt;/code&gt; unit - so the optimal partition is trivially a set with all 26 letters. However, we start the program with smaller partitions to demonstrate that the column generation ends up with the optimal solution.&lt;/p&gt;
&lt;p&gt;Denote &lt;code&gt;X&lt;/code&gt; is the set of elements and &lt;code&gt;K&lt;/code&gt; is a initial set of partitions. These can be arbitrary subsets.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import string
X = list(string.ascii_lowercase)
K = [X[i:i + 5] for i in range(0, len(X), 5)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s start with the (restricted) master program which is formulated as a linear program.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def master(X: List, K: List[List]) -&amp;gt; cplex.Cplex:
    &amp;quot;&amp;quot;&amp;quot; Restricted master problem &amp;quot;&amp;quot;&amp;quot;

    prob = cplex.Cplex()
    numvar = len(K)

    names = list(map(func, K))
    var_type = [prob.variables.type.continuous] * numvar
    prob.variables.add(names=names,
                    lb=[0.0] * numvar,
                    ub=[1.0] * numvar,
                    types=var_type)
    prob.objective.set_sense(prob.objective.sense.minimize)
    prob.objective.set_linear([(n, cost(kj)) for n, kj in zip(names, K)])

    lhs = []
    for i in X:
        coeffs = [1.0 if i in kj  else 0.0 for kj in K]
        lhs.append(cplex.SparsePair(names, coeffs))
    prob.linear_constraints.add(lin_expr=lhs,
                                rhs=[1.0] * len(lhs),
                                senses=[&#39;E&#39;] * len(lhs),
                                names=X)
    prob.set_problem_type(cplex.Cplex.problem_type.LP)
    print(f&amp;quot;{len(lhs)} constraints and {len(names)} variables.&amp;quot;)

    return prob
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The column generation procedure will solve the master at every iteration. Retrieve the dual variable values and then use that in a pricing step to determine a new partition to add (more on this next). If the reduced cost is negative then the objective will improve by the addition. If not, then no new partition has been found to improve the solution and we can terminate.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;while True:
    p.solve()
    pi = [-p for p in p.solution.get_dual_values()]

    # Generate new partitions/columns
    K_new, rc_new = pricing(X, pi)

    # termination check
    if rc_new &amp;lt;= -eps:

        newvar = func(K_new)
        print(f&amp;quot;Iteration {i}: Adding &#39;{&#39;&#39;.join(K_new)}&#39; column with rc: {rc_new}.&amp;quot;)

        p.variables.add(names=[newvar],
                        lb=[0.0], ub=[1.0],
                        types=[p.variables.type.continuous])
        p.objective.set_linear(newvar, cost(K_new))
        for c in K_new:
            p.linear_constraints.set_coefficients(str(c), newvar, 1.0)

        p.set_problem_type(cplex.Cplex.problem_type.LP)
        i+=1
    else:
        print(&amp;quot;No improvement partition found - Terminating column generation.&amp;quot;)
        break
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now on to the pricing step - the step where new columns are generated. Here is where, in actual applications, the heavy lifting would happen. For this simple example, we can just generate partitions with negative duals and compute the reduced cost of this partition. In many cases, this can be a combinatorial problem (resulting in another integer program) or have some structure that can be leveraged (e.g. a shortest path or allow for dynamic programming solutions).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def pricing(X: List, pi: List, ncols: int=5) -&amp;gt; Tuple[List, float]:
    &amp;quot;&amp;quot;&amp;quot; Heuristic to generate new partitions &amp;quot;&amp;quot;&amp;quot;
    
    # set of elements that have negative dual variables
    K_new = [x for p, x in sorted(zip(pi, X), reverse=True) if p &amp;lt; 0.0]
    
    # reduced cost of new partition
    rc_new = rc(K_new, pi, X)

    return sorted(K_new), rc_new
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;putting-it-together&#34;&gt;Putting it together&lt;/h2&gt;
&lt;p&gt;Running this together as shown in this 
&lt;a href=&#34;https://gist.github.com/rahulnair23/b3be98553df6637f7b7fd4490e80991d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gist&lt;/a&gt; gives the following output:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;26 constraints and 3 variables.
Iteration 0: Adding &#39;aku&#39; column with rc: -2.0.
Iteration 1: Adding &#39;bku&#39; column with rc: -3.0.
Iteration 2: Adding &#39;ablu&#39; column with rc: -2.5.
Iteration 3: Adding &#39;abklv&#39; column with rc: -2.25.
Iteration 4: Adding &#39;ckluv&#39; column with rc: -2.2857142857142856.
Iteration 5: Adding &#39;abckmuv&#39; column with rc: -2.307692307692308.
Iteration 6: Adding &#39;abckluw&#39; column with rc: -2.736842105263158.
Iteration 7: Adding &#39;aclmvw&#39; column with rc: -6.777777777777778.
Iteration 8: Adding &#39;bdlmuvw&#39; column with rc: -3.2222222222222223.
Iteration 9: Adding &#39;cdkvw&#39; column with rc: -5.000000000000001.
Iteration 10: Adding &#39;acdklmuvx&#39; column with rc: -2.4818181818181815.
Iteration 11: Adding &#39;abdkmwx&#39; column with rc: -5.540540540540541.
Iteration 12: Adding &#39;bclmx&#39; column with rc: -8.879999999999999.
...
Iteration 174: Adding &#39;abcdefghijklmnopqrstuwxy&#39; column with rc: -0.10279605263157898.
Iteration 175: Adding &#39;abcdefghijklmnopqrstuvwxyz&#39; column with rc: -0.08656995788488508.
Optimal value: 1.0
Partition: &#39;abcdefghijklmnopqrstuvwxyz&#39; with cost: 1.0.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since each partition costs one unit, the optimal partition is one single partition.&lt;/p&gt;
&lt;p&gt;The number of possible partitions of a set is known as the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Bell_number&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bell number&lt;/a&gt;. For the 26 alphabet, the total number of possible partitions is very large (&lt;code&gt;49631246523618756274&lt;/code&gt; as per &lt;code&gt;sympy.bell&lt;/code&gt; in Python). In this example the procedure evaluated 175 partitions before arriving at the optimal one.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Model explanations via column generation</title>
      <link>https://rahulnair23.github.io/post/boolean-rules/</link>
      <pubDate>Tue, 16 Jun 2020 10:46:12 +0100</pubDate>
      <guid>https://rahulnair23.github.io/post/boolean-rules/</guid>
      <description>&lt;p&gt;In this post, I&amp;rsquo;ll review a paper from 2018 that deals with generating boolean decision rules and uses column generation. The 
&lt;a href=&#34;https://dl.acm.org/doi/10.5555/3327345.3327376&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; is well worth the read if you are interested in explainable AI models. The work also won the 
&lt;a href=&#34;https://community.fico.com/s/explainable-machine-learning-challenge&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FICO explainability challenge&lt;/a&gt; by applying this method to data from the financial services sector.&lt;/p&gt;
&lt;h1 id=&#34;setting&#34;&gt;Setting&lt;/h1&gt;
&lt;p&gt;Algorithmic decisions - ones where you rely on machines to reach a conclusion - require justification. Why was my loan approval denied? Why was the scan result classified as cancerous? In these and several other critical sectors, simply stating the prediction of a AI system is not enough. The underlying rationale of &amp;ldquo;why&amp;rdquo; is equally important.&lt;/p&gt;
&lt;p&gt;Several methods seek to come up with the justification algorithmically (in a field called Explainable AI) and several methods exist that can be distinguished along several dimensions. The main one is scope. The &lt;em&gt;global&lt;/em&gt; methods look to explain the entire model, i.e. how does the model behave? This contrasts with &lt;em&gt;local&lt;/em&gt; methods that look to explain a single instance, i.e. why was my loan not approved?&lt;/p&gt;
&lt;p&gt;When the models are complex, as is often the case, a popular class of methods are surrogate models. The basic goal here is to build a simpler (e.g. linear) version of a complex model, then look to interpret the simpler model in ways we humans can understand. When built for &lt;em&gt;local&lt;/em&gt; explanations, these probe the neighborhood of test instance to build a surrogate of the complex model.&lt;/p&gt;
&lt;p&gt;Unfortunately, it is hard to be objective when it comes to explanations. In practice, different methods will result in vastly different explanations for the same instance on the same underlying (complex) model. For minor changes in training samples or an adjustment of parameters, even the same method can give you very different results. Fundamentally, these approaches are limited by design. If you prescribe to the view that a machine learning (ML) model is all but a lossy compressed view on the data, then surrogate models are a lossy compressed view of the ML model. Significant challenges remain in practical deployments.&lt;/p&gt;
&lt;p&gt;An alternative view, the one described in the paper, is one where the model is directly interpretable. A directly interpretable model is one that can be understood by humans. There are of course, several models like decision trees that fall in this category.&lt;/p&gt;
&lt;h1 id=&#34;the-problem&#34;&gt;The problem&lt;/h1&gt;
&lt;p&gt;Consider a supervised binary classification task. You are given $(X_i, y_i)$ for observations $i = 1,&amp;hellip;, n$, where $X_i$ is the set of features associated with observation $i$ and $y_i$ is the binary outcome label. The task is to build a boolean classifier $\hat{y}(\mathbf{x})$ that can be stated as&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if (condition) then (predict True) else (predict False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;(condition)&lt;/code&gt; is of a specific form called a Disjunctive Normal Form (DNF). DNF clauses are OR of ANDs. A DNF clause on when to drink beer would be &lt;code&gt;(mood=HAPPY AND inventory&amp;gt;0) OR (mood=SAD AND inventory&amp;gt;0) OR (temp&amp;gt;15°C)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For a dataset there are exponentially many such clauses involving its features. The challenge is to find a relatively compact subset that provides the best prediction accuracy. The condition needs to be compact as overly complex clauses are not interpretable.&lt;/p&gt;
&lt;h1 id=&#34;the-model&#34;&gt;The model&lt;/h1&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://dl.acm.org/doi/10.5555/3327345.3327376&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; formulates the search for these clauses as a mixed-integer programming problem.&lt;/p&gt;
&lt;p&gt;$P$ denotes the set of positive samples, i.e. the observations where $y_i = 1$ and $Z$ denote negative ones. All features in $X_i$ are assumed to be binary valued. This isn&amp;rsquo;t too restrictive, continuous and categorical data can be encoded this way.  There is $K$ a set of (exponentially many) clauses involving features of $X$ and $K_i, K_i\subseteq K$ is the subset of clauses satisfied by observation $i$.&lt;/p&gt;
&lt;p&gt;There are two decision variables. First is $w_k$ (for all $k$ in set $K$) - a binary variable on if clause $k$ is selected for the model. Each clause $k$ in $K$ has an associated complexity $c_k$. The second is $\xi_i$ defined for $i \in P$ (i.e. for all positive samples) denotes all samples that are classified incorrectly.&lt;/p&gt;
&lt;p&gt;The objective looks to minimize Hamming loss which is the fraction of misclassified samples. Specifically, this can be written as&lt;/p&gt;
&lt;p&gt;$$
\min_{\xi, w} \color{blue}\underbrace{\color{black}\sum_{i\in P} \xi_i}&lt;em&gt;{\text{false negatives}} {\color{black}+}
\color{blue}\underbrace{\color{black}\sum&lt;/em&gt;{i\in Z}\sum_{k\in K_i} w_k}_{\text{false positives}}
$$&lt;/p&gt;
&lt;p&gt;False positives add more than &amp;ldquo;one unit&amp;rdquo; if multiple clauses are satisfied. This is&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
\mbox{s.t.} \qquad &amp;amp; \xi_i + \sum_{k\in K_i} w_k \ge 1 \qquad \xi_i \ge 0, \qquad i\in P \\
&amp;amp; \sum_{k\in K} c_k w_k \le C \\
&amp;amp; w_k \in \{0, 1\} \qquad \forall k\in K
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The first constraint looks to identify false negatives. It basically says, for each positive sample, either chalk up a false negative ($\xi_i$) or include a rule that correctly represents this observation (i.e. a clause from the set $K_i$). The second constraint simply bounds the total complexity of the selected rule set to a parameter $C$.&lt;/p&gt;
&lt;p&gt;The overall problem still remains however, the set $K$ is very large and we&amp;rsquo;d like to avoid having to generate the entire set. This mainly because, it is expensive to generate and solve for all $K$. In any case, only a few $w_k$ will be selected in the final solution, so it makes sense to only look at clauses as needed.&lt;/p&gt;
&lt;p&gt;In the maximum clique 
&lt;a href=&#34;../max-clique&#34;&gt;post&lt;/a&gt; earlier, this type of exponential growth was addressed using constraint generation. In this work, they follow a column generation procedure - one of generating new variables ($w_k$) as needed. Adding rows and columns to a optimization program are strongly coupled topics. Adding a constraint (row) to a program is  the same as adding a variable (column) to its dual.&lt;/p&gt;
&lt;h2 id=&#34;the-pricing-problem&#34;&gt;The pricing problem&lt;/h2&gt;
&lt;p&gt;Now to the problem of generating new conjunctive clauses. The sub-problem looks to find the missing clause with the highest reduced cost, i.e. the clause that has the greatest impact on the objective function. This is a heuristic selection and you may have to undo a selection at a later step. But if no missing clause with a negative reduced cost can be found, the procedure terminates with the optimal solution.&lt;/p&gt;
&lt;p&gt;A bit more notation. From the program above, take $\mu_i$ to be the dual variables associated with the first constraint, and $\lambda$ be the dual variable for the complexity constraint. Define two sets of decision variables $\delta_i$ for all observations $i$ and $z_j$ if feature $j\in J$ is selected in missing constraint. Additionally denote $S_i$ to be a set of zero valued features for sample $i$.&lt;/p&gt;
&lt;p&gt;Armed with this, the sub-problem to identify a clause to include (with the greatest negative reduce cost) can be formulated as:&lt;/p&gt;
&lt;p&gt;$$
\min_{\delta, z} \color{blue}\underbrace{\color{black}\lambda\left(1+\sum_{j\in J}z_j\right)}&lt;em&gt;{\substack{\text{complexity of new rule}\\\text{in terms of features selected}}} {\color{black}-}
\color{blue}\underbrace{\color{black}\sum&lt;/em&gt;{i\in P} \mu_i \delta_i}&lt;em&gt;{\substack{\text{how much the new rule}\\\text{improves false negatives}}}{\color{black}+}
\color{blue}\underbrace{\color{black}\sum&lt;/em&gt;{i\in Z} \delta_i}_{\substack{\text{how much the new rule}\\\text{hurts false positives}}}
$$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
\mbox{s.t.} \qquad &amp;amp; \delta_i + z_j\le 1 \qquad j\in S_i, \qquad i\in P \\
&amp;amp;  \delta_i \ge 1- \sum_{j\in S_i} z_j \qquad \delta_i\ge 0 \qquad i\in Z\\
&amp;amp; \sum_{j\in J} z_j \le D \\
&amp;amp; z_j \in \{0, 1\} \qquad \forall j\in J
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The first constraint here says every zero-valued feature in every positive sample - the feature is either selected or the sample satisfies the clause. The next constraint states that the $i$-th negative sample is covered only if no zero-valued feature is selected. The next constraint bounds the complexity and the feature selection variables are restricted to be binary.&lt;/p&gt;
&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;The method appears to be well suited for transactional-type data, where there are some underlying rules (e.g. a business process) that generates the training data. I leave you with an interesting example. The tic-tac-toe 
&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Tic-Tac-Toe&amp;#43;Endgame&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dataset&lt;/a&gt; contains all possible board configurations at the end of a 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Tic-tac-toe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tic-tac-toe game&lt;/a&gt; and the outcome (if &lt;code&gt;x&lt;/code&gt; won or not). The method extracts almost all the rules of the game based on just this data, just missing the diagonal cases. Some of the rules extracted are superfluous.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-rules-extracted-from-tic-tac-toe-data&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://rahulnair23.github.io/post/boolean-rules/tictactoe_hu1ec8a325d060e5397d7307828d9e84ef_98406_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Rules extracted from tic-tac-toe data&#34;&gt;


  &lt;img data-src=&#34;https://rahulnair23.github.io/post/boolean-rules/tictactoe_hu1ec8a325d060e5397d7307828d9e84ef_98406_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1270&#34; height=&#34;618&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Rules extracted from tic-tac-toe data
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;As with all things, the approach comes with some limitations. There is a complexity-accuracy tradeoff, expect classifier accuracy to drop. Although in my experiments this was not substantial. The clauses generated are sensitive to the parameter $C$. If you don&amp;rsquo;t have a mechanism to validate the rule set then it is difficult to tune. The method doesn&amp;rsquo;t deal with class-imbalance, so you would need to under/over-sample to get a balanced sample. It would also worthwhile to study clauses generated in the presence of highly correlated features. I expect one of the correlated features to be picked arbitrarily.&lt;/p&gt;
&lt;h1 id=&#34;resources&#34;&gt;Resources&lt;/h1&gt;
&lt;p&gt;A modified implementation of the model is available through the 
&lt;a href=&#34;https://github.com/IBM/AIX360/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AIX360&lt;/a&gt; toolkit, so you can try it yourself. The differences are documented in another 
&lt;a href=&#34;https://arxiv.org/pdf/1909.03012.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;. Mainly, it uses a beam search heuristic instead of the pricing problem. Additionally, the complexity clauses are handled through two regularization terms, rather than a constraint.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
