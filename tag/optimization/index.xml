<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>optimization | Rahul Nair</title>
    <link>https://rahulnair23.github.io/tag/optimization/</link>
      <atom:link href="https://rahulnair23.github.io/tag/optimization/index.xml" rel="self" type="application/rss+xml" />
    <description>optimization</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 30 Apr 2021 13:26:35 +0100</lastBuildDate>
    <image>
      <url>https://rahulnair23.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>optimization</title>
      <link>https://rahulnair23.github.io/tag/optimization/</link>
    </image>
    
    <item>
      <title>Partitioning a set</title>
      <link>https://rahulnair23.github.io/post/set-partition/</link>
      <pubDate>Fri, 30 Apr 2021 13:26:35 +0100</pubDate>
      <guid>https://rahulnair23.github.io/post/set-partition/</guid>
      <description>&lt;p&gt;In this post, we summarize the set partitioning problem, a widely applicable formalization that can be used in a variety of contexts. You are given a set of elements that need to be divided. Each division, or partition, has an associated cost that you seek to minimize. The set partitioning problem asks: what is the best way to partition such that the total cost can be minimized?&lt;/p&gt;
&lt;p&gt;This abstraction applied to a broad range of cases. From cutting cake for your family (if you consider cost to decrease as you get a larger piece) to more industrial applications like scheduling and crew rostering. There is also a large literature on the problem. A 
&lt;a href=&#34;https://doi.org/10.1137/1018115&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;survey&lt;/a&gt; paper was published way back in 1976 and reference on 
&lt;a href=&#34;https://doi.org/10.1007/978-1-4613-0303-9_9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;applications&lt;/a&gt; in 1998.&lt;/p&gt;
&lt;h2 id=&#34;the-problem&#34;&gt;The problem&lt;/h2&gt;
&lt;p&gt;In the basic setting, we have a set $X$ of elements and a collection of possible partitions $K={k_1, k_2,&amp;hellip;,k_m}$ and a cost function $c(k_j)$ associated with each partition. $K$ is a very large collection as there are several ways to divide a set. We define a membership matrix $A = [a_{ij}]$ where $a_{ij}=1$ if element $i$ from $X$ is in the partition $k_j$ and $0$ otherwise. We denote $x_j$ as a decision variable to decide if partition $k_j$ is employed/used or not. With these definitions, the set partitioning problem can be written as:&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
\min_{x} \quad &amp;amp; \sum_{j=1}^{n} c(k_j) x_j\\
\textrm{s.t.} \quad &amp;amp; \sum_{j=1}^{m} a_{ij}x_j=1 \quad \forall i=1,&amp;hellip;,n\\
&amp;amp;x_j\in {0,1}    \\
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The program seeks to find a set of partitions that divide the input feature space such that every element is covered by exactly one partition. (If we allow for overlap between partitions, we could use a coverage type constraint $\sum a_{ij}x_j \ge 1$ as well). The objective is to find a partitioning that minimizes the total cost of the partition.&lt;/p&gt;
&lt;h2 id=&#34;column-generation&#34;&gt;Column generation&lt;/h2&gt;
&lt;p&gt;The model above can not typically be solved directly for realistic sized instances. This is because the collection $K$ is very large. For cases where the number of columns is much greater than the rows, i.e. $m\gg n$, we can start with a restricted set of partitions and generate partitions on the fly as needed. The method of column generation is useful here to do this in a principled manner and can work as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Restricted master problem&lt;/strong&gt;: We start with a limited set of partitions $J, J\subset K$ and relax integral conditions to solve a linear program. Denote $\pi_i$ as the dual variables associated with each equality constraint. We solve this limited problem and recover the values of $\pi_i, i=1, &amp;hellip;, n$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generate new partitions&lt;/strong&gt;: Formulate a pricing problem that seeks to find partition that can have the largest benefit, in terms of the real objective.  The general approach is to find a column (i.e. partition) that has the largest negative 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Reduced_cost&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reduced cost&lt;/a&gt;. If $z_i$ as a binary decision variable that denotes if element $i$ is in the new partition, the problem of finding a new partition can be stated as&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\begin{aligned}
\min_{z} \quad &amp;amp; c(z) - \sum_{i=1}^{n} \pi_i z_i\\
&amp;amp;z_i\in {0,1}    \\
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where $c(z)$ denotes the cost of the chosen partition. Depending on your problem specifics and the nature of this function the manner in which you solve this would vary. Looking for the column with the greatest negative reduced cost is a heuristic step and can be replaced by other procedures to generate new candidate partitions if you have some insights that help you to do so.&lt;/p&gt;
&lt;h2 id=&#34;reference-implementation&#34;&gt;Reference implementation&lt;/h2&gt;
&lt;p&gt;To build some intuition, let&amp;rsquo;s look at a reference implementation for a trivial toy example. The objective is to find the optimal partition of the English alphabet. The cost of each partition is &lt;code&gt;1.0&lt;/code&gt; unit - so the optimal partition is trivially a set with all 26 letters. However, we start the program with smaller partitions to demonstrate that the column generation ends up with the optimal solution.&lt;/p&gt;
&lt;p&gt;Denote &lt;code&gt;X&lt;/code&gt; is the set of elements and &lt;code&gt;K&lt;/code&gt; is a initial set of partitions. These can be arbitrary subsets.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import string
X = list(string.ascii_lowercase)
K = [X[i:i + 5] for i in range(0, len(X), 5)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s start with the (restricted) master program which is formulated as a linear program.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def master(X: List, K: List[List]) -&amp;gt; cplex.Cplex:
    &amp;quot;&amp;quot;&amp;quot; Restricted master problem &amp;quot;&amp;quot;&amp;quot;

    prob = cplex.Cplex()
    numvar = len(K)

    names = list(map(func, K))
    var_type = [prob.variables.type.continuous] * numvar
    prob.variables.add(names=names,
                    lb=[0.0] * numvar,
                    ub=[1.0] * numvar,
                    types=var_type)
    prob.objective.set_sense(prob.objective.sense.minimize)
    prob.objective.set_linear([(n, cost(kj)) for n, kj in zip(names, K)])

    lhs = []
    for i in X:
        coeffs = [1.0 if i in kj  else 0.0 for kj in K]
        lhs.append(cplex.SparsePair(names, coeffs))
    prob.linear_constraints.add(lin_expr=lhs,
                                rhs=[1.0] * len(lhs),
                                senses=[&#39;E&#39;] * len(lhs),
                                names=X)
    prob.set_problem_type(cplex.Cplex.problem_type.LP)
    print(f&amp;quot;{len(lhs)} constraints and {len(names)} variables.&amp;quot;)

    return prob
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The column generation procedure will solve the master at every iteration. Retrieve the dual variable values and then use that in a pricing step to determine a new partition to add (more on this next). If the reduced cost is negative then the objective will improve by the addition. If not, then no new partition has been found to improve the solution and we can terminate.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;while True:
    p.solve()
    pi = [-p for p in p.solution.get_dual_values()]

    # Generate new partitions/columns
    K_new, rc_new = pricing(X, pi)

    # termination check
    if rc_new &amp;lt;= -eps:

        newvar = func(K_new)
        print(f&amp;quot;Iteration {i}: Adding &#39;{&#39;&#39;.join(K_new)}&#39; column with rc: {rc_new}.&amp;quot;)

        p.variables.add(names=[newvar],
                        lb=[0.0], ub=[1.0],
                        types=[p.variables.type.continuous])
        p.objective.set_linear(newvar, cost(K_new))
        for c in K_new:
            p.linear_constraints.set_coefficients(str(c), newvar, 1.0)

        p.set_problem_type(cplex.Cplex.problem_type.LP)
        i+=1
    else:
        print(&amp;quot;No improvement partition found - Terminating column generation.&amp;quot;)
        break
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now on to the pricing step - the step where new columns are generated. Here is where, in actual applications, the heavy lifting would happen. For this simple example, we can just generate partitions with negative duals and compute the reduced cost of this partition. In many cases, this can be a combinatorial problem (resulting in another integer program) or have some structure that can be leveraged (e.g. a shortest path or allow for dynamic programming solutions).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def pricing(X: List, pi: List, ncols: int=5) -&amp;gt; Tuple[List, float]:
    &amp;quot;&amp;quot;&amp;quot; Heuristic to generate new partitions &amp;quot;&amp;quot;&amp;quot;
    
    # set of elements that have negative dual variables
    K_new = [x for p, x in sorted(zip(pi, X), reverse=True) if p &amp;lt; 0.0]
    
    # reduced cost of new partition
    rc_new = rc(K_new, pi, X)

    return sorted(K_new), rc_new
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;putting-it-together&#34;&gt;Putting it together&lt;/h2&gt;
&lt;p&gt;Running this together as shown in this 
&lt;a href=&#34;https://gist.github.com/rahulnair23/b3be98553df6637f7b7fd4490e80991d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gist&lt;/a&gt; gives the following output:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;26 constraints and 3 variables.
Iteration 0: Adding &#39;aku&#39; column with rc: -2.0.
Iteration 1: Adding &#39;bku&#39; column with rc: -3.0.
Iteration 2: Adding &#39;ablu&#39; column with rc: -2.5.
Iteration 3: Adding &#39;abklv&#39; column with rc: -2.25.
Iteration 4: Adding &#39;ckluv&#39; column with rc: -2.2857142857142856.
Iteration 5: Adding &#39;abckmuv&#39; column with rc: -2.307692307692308.
Iteration 6: Adding &#39;abckluw&#39; column with rc: -2.736842105263158.
Iteration 7: Adding &#39;aclmvw&#39; column with rc: -6.777777777777778.
Iteration 8: Adding &#39;bdlmuvw&#39; column with rc: -3.2222222222222223.
Iteration 9: Adding &#39;cdkvw&#39; column with rc: -5.000000000000001.
Iteration 10: Adding &#39;acdklmuvx&#39; column with rc: -2.4818181818181815.
Iteration 11: Adding &#39;abdkmwx&#39; column with rc: -5.540540540540541.
Iteration 12: Adding &#39;bclmx&#39; column with rc: -8.879999999999999.
...
Iteration 174: Adding &#39;abcdefghijklmnopqrstuwxy&#39; column with rc: -0.10279605263157898.
Iteration 175: Adding &#39;abcdefghijklmnopqrstuvwxyz&#39; column with rc: -0.08656995788488508.
Optimal value: 1.0
Partition: &#39;abcdefghijklmnopqrstuvwxyz&#39; with cost: 1.0.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since each partition costs one unit, the optimal partition is one single partition.&lt;/p&gt;
&lt;p&gt;The number of possible partitions of a set is known as the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Bell_number&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bell number&lt;/a&gt;. For the 26 alphabet, the total number of possible partitions is very large (&lt;code&gt;49631246523618756274&lt;/code&gt; as per &lt;code&gt;sympy.bell&lt;/code&gt; in Python). In this example the procedure evaluated 175 partitions before arriving at the optimal one.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Zero-Suppressed Decision Diagrams and Independent Sets</title>
      <link>https://rahulnair23.github.io/post/decision-diagrams/</link>
      <pubDate>Tue, 26 Jan 2021 13:16:32 +0000</pubDate>
      <guid>https://rahulnair23.github.io/post/decision-diagrams/</guid>
      <description>&lt;p&gt;I stumbled across Binary Decision Diagrams (BDDs) by chance. They are an efficient data structure to represent sets of graphs. While a graph $G$ is a set of vertices $V$ along with a set of edges $E$ that connect the vertices, a graph set is a collection of subgraphs over the universe $V$. For example, a collection of feasible paths on $G$ would be a graph set. For a graph, if one has its corresponding BDD, specific types of queries can be handled with remarkable efficiency. See the 
&lt;a href=&#34;https://youtu.be/Q4gTV4r0zRs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; video from the creators of 
&lt;a href=&#34;https://github.com/takemaru/graphillion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;grahillion&lt;/a&gt; for an illustration of count queries. Knuth&amp;rsquo;s Art of Programming has an entire chapter devoted to this, but it hasn&amp;rsquo;t seen too many uses in the operations research community (at least as far as I&amp;rsquo;m aware). It seems like it could be pretty useful for several combinatorial optimization problems.&lt;/p&gt;
&lt;p&gt;So in this post, we&amp;rsquo;ll work through one use - that of finding maximal independent sets in a graph. An independent set is a subset of vertices $V$, no two of which are adjacent. A &lt;em&gt;maximal&lt;/em&gt; independent set is one where no additional nodes can be included and remain as an independent set. Not to be confused with the maxim&lt;strong&gt;um&lt;/strong&gt; set - which looks for an independent set with maximum weight.&lt;/p&gt;
&lt;p&gt;We follow the work described in 
&lt;a href=&#34;https://doi.org/10.1007/s10878-014-9722-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Morrison et al. (2014)&lt;/a&gt; on this problem which uses a Zero-suppressed Decision Diagram (ZDD), a variant of BDD where the false conditions are omitted. This is useful when the feasible solutions are sparse. There are several works that look into using ZDDs on graph and optimization problems, e.g. maximal cliques 
&lt;a href=&#34;https://doi.org/10.1109/EDTC.1997.582363&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Coudert, 1997)&lt;/a&gt; and 
&lt;a href=&#34;http://people.mpi-inf.mpg.de/alumni/d1/2019/behle/azove.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;0/1 enumeration&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;zdds&#34;&gt;ZDDs&lt;/h1&gt;
&lt;p&gt;In addition to a graph $G$, we also need a vertex ordering. While the ordering can be arbitrary, it does have an impact on the resulting ZDD. Each node in the ZDD, $Z_s$ is defined by a tuple $(v, lo, hi)$, where $v$ is the vertex it refers to in $G$ and $lo, hi$ are exactly two branches, known as the low and high branch, pointers to other nodes in the ZDD. These can be thought of as binary outcomes of the node from which they emanate. The terminal nodes in the ZDD have special meaning. The $\top$ refers to a &lt;strong&gt;true&lt;/strong&gt; outcome, while $\bot$ encodes a &lt;strong&gt;false&lt;/strong&gt; outcome.&lt;/p&gt;
&lt;p&gt;All this becomes clearer in the example, taken from Morrison&amp;rsquo;s paper, below. The 5-node graph on the left has a corresponding ZDD on the right that encodes all possible maximal independent sets. The solid arrows are high branches, while the dashed ones are low branches.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig.svg&#34; alt=&#34;Alt&#34;&gt;&lt;/p&gt;
&lt;p&gt;This Directed Acyclic Graph (DAG) encodes every possible maximal independent set. Every feasible path from the root node till the terminal node $\top$ represents a maximal independent set. The set it represented by nodes that have high branches coming out. For example, feasible path $v_1, v_2, v_3, v_4,\top$ represents an maximal independent set ${v_2, v_4}$ as these are the only nodes with solid (high) branches. There are several important properties of the ZDD that I skip here, like the uniqueness of nodes in $Z_s$, each node is encountered only once, etc.&lt;/p&gt;
&lt;p&gt;The ZDD can now be used to enumerate all maximal independent sets and support non-trivial operations that would be expensive otherwise, e.g. a count of all maximal independent sets without listing each one. There are other advantages as well. For instance, if there is a need to repeatedly query this ZDD, for example when node weights change, then one can compute this efficiently. Mostly, ZDD offer a compact representation of the underlying set.&lt;/p&gt;
&lt;h1 id=&#34;constructing-a-zdd&#34;&gt;Constructing a ZDD&lt;/h1&gt;
&lt;p&gt;To do all of this, the ZDD needs to be constructed first. There are few methods to do this for each class of problem you want to solve. 
&lt;a href=&#34;https://doi.org/10.1007/s10878-014-9722-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Morrison et al. (2014)&lt;/a&gt;, for example, tackle the case of the maximal independent set.&lt;/p&gt;
&lt;p&gt;In this case, they describe an algorithm that takes as input a graph and a vertex ordering (this ordering can be arbitrary - but does have an influence on the size of the ZDD). The procedure is recursive, and it each iteration a single node is considered. It first considers if a maximal set can be constructed from the remaining vertices. Remaining here are w.r.t. to the ordering. It also checks if the set is already maximal. For these two cases, the procedure returns with the terminal nodes, i.e. $\bot$ or $\top$. If these conditions are not met, it creates two branches for the current node based on the next undominated node. A new node is added if it hasn&amp;rsquo;t previously been created.&lt;/p&gt;
&lt;h1 id=&#34;reference-implementation&#34;&gt;Reference implementation&lt;/h1&gt;
&lt;p&gt;A reference implementation is forthcoming (when time permits!).&lt;/p&gt;
&lt;p&gt;There are existing packages like 
&lt;a href=&#34;https://github.com/takemaru/graphillion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;graphmillion&lt;/a&gt; that provide powerful abstractions for common problem classes. See also their accompanying 
&lt;a href=&#34;https://doi.org/10.1007/s10009-014-0352-z&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;. 
&lt;a href=&#34;http://people.mpi-inf.mpg.de/alumni/d1/2019/behle/azove.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Azove&lt;/a&gt; is another tool (uses binary decision diagrams) for vertex enumeration that may be of interest as well.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Maximum weighted cliques in a graph</title>
      <link>https://rahulnair23.github.io/post/max-clique/</link>
      <pubDate>Fri, 05 Jun 2020 10:44:59 +0100</pubDate>
      <guid>https://rahulnair23.github.io/post/max-clique/</guid>
      <description>&lt;p&gt;Recently, I had the need to compute maximum weighted cliques on very dense large graphs. This is a well studied problem, and a nice 
&lt;a href=&#34;https://doi.org/10.1007/BF01098364&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;survey paper&lt;/a&gt; from 90&amp;rsquo;s by Pardalos and Xue gives a good overview of approaches.&lt;/p&gt;
&lt;h2 id=&#34;the-problem&#34;&gt;The problem&lt;/h2&gt;
&lt;p&gt;We are given a graph $G = (V, E)$ which is a set of vertices $V$ and edges $E$. Each vertex has an associated weight $d_i, \forall i\in V$. A &lt;em&gt;clique&lt;/em&gt; $C$ is a subset of vertices, where all vertices are pairwise connected. A &lt;em&gt;maximum&lt;/em&gt; clique is a clique that has largest weight.&lt;/p&gt;
&lt;p&gt;A related notion is of an &lt;em&gt;independent set&lt;/em&gt;, which is a subset of vertices $V$ that are pairwise disconnected. A maximum independent set is similarly an independent set with the largest weight.&lt;/p&gt;
&lt;h2 id=&#34;solutions&#34;&gt;Solutions&lt;/h2&gt;
&lt;p&gt;For general graphs, finding the maximum cliques is a hard problem (NP-complete). An integer programming approach that involves edges can be written as:&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
\max_x &amp;amp; \sum_{i\in V} d_i x_{i}\\
\mbox{s.t.} \qquad &amp;amp; x_i + x_j \le 1 \quad \forall (i, j) \in \bar{E} \\
&amp;amp; x_i \in {0, 1} \qquad \forall i\in V
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where $\bar{E}$ is called the complement edge set that is a set of edges that are missing from the original graph $G$. All the constraint $x_i + x_j \le 1$ says is if the edge $(i, j)$ is missing then only one node, either $i$ or $j$, can be in the clique. The objective seeks to maximize the weighted of selected nodes.&lt;/p&gt;
&lt;p&gt;As pointed out in the 
&lt;a href=&#34;https://doi.org/10.1007/BF01098364&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; this formulation isn&amp;rsquo;t useful in practice on account of two problems. First, the linear relaxation, one where the integrality constraints $x_i \in {0, 1}$ is omitted, gives a poor bound. The second is on account of symmetry. Symmetry arises in this context as vertices with the same weight are indistinguishable. So several configurations result in the same optimal solution. Why is this bad? The branch and cut tree cannot prune the search tree as solutions are in various parts of it. One way to remove symmetry is to do lexicographical ordering. An arbitrary order is imposed via additional constraints that cuts of several optimal solutions knowing that at least one optimal solution is valid. There are other methods as well, such as 
&lt;a href=&#34;https://doi.org/10.1007/s10107-002-0358-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;isomorphic pruning&lt;/a&gt;, and 
&lt;a href=&#34;https://doi.org/10.1016/j.disopt.2011.07.001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;orbitopal fixing&lt;/a&gt;, but we won&amp;rsquo;t get into those here.&lt;/p&gt;
&lt;h2 id=&#34;an-alternative-formulation&#34;&gt;An alternative formulation&lt;/h2&gt;
&lt;p&gt;The notion of the complement edge set $\bar{E}$ can be strengthened using independent sets. If you know an independent set, a clique can contain at most one vertex from such a set. To write this as a constraint, one would need to consider &lt;em&gt;maximum&lt;/em&gt; independent sets, lest you allow the omitted vertices to be included in the clique. Further, you would need to look at all maximum independent sets that arise in $G$. Assume for a moment that the set of maximum independent sets $\mathbb{S}$ is known. Then the problem of finding the maximum weighted clique can be written as&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
\max_x &amp;amp; \sum_{i\in V} d_i x_{i}\\
\mbox{s.t.} \qquad &amp;amp; \sum_{i\in I} x_i \le 1 \quad \forall I \in \mathbb{S} \\
&amp;amp; x_i \in {0, 1} \qquad \forall i\in V
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;The objective is the same as before - maximize the total weight of selected nodes. The constraints, one for each maximum independent set, allows only one vertex into the solution. The constraints are tighter than the previous formulation implying that the linear relaxation gives a better bound. For some classes of graphs, namely perfect graphs, omitting the integrality constraints will directly give you integral solutions! The problem however is that there are now an exponential number of constraints (set $\mathbb{S}$ is very large).&lt;/p&gt;
&lt;p&gt;One mechanism to deal with too many constraints is use lazy constraints. The idea is to start the optimization with a small set of constraints and then add &lt;em&gt;relevant&lt;/em&gt; constraints from the large pool as you go along. The prerequisite is however that such relevant constraints can be generated efficiently.&lt;/p&gt;
&lt;p&gt;How would this work? A sketch of the solution algorithm looks like this.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Use a heuristic procedure to generate a set of maximum independent sets ($S$)&lt;/li&gt;
&lt;li&gt;Solve a linear relaxation on this limited set ($S \subseteq \mathbb{S}$)&lt;/li&gt;
&lt;li&gt;Based on solution, identify new maximum independent sets that would cut the current solution&lt;/li&gt;
&lt;li&gt;If no such sets exists, we are done (current solution gives the maximum weighted clique)&lt;/li&gt;
&lt;li&gt;If there are, then add to the constraints and goto step 2.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;reference-implementation&#34;&gt;Reference implementation&lt;/h2&gt;
&lt;p&gt;To test this, we use the &lt;code&gt;networkx&lt;/code&gt; library for graphs and the CPLEX solver. We use one of the many generators for a test graph with a known number of cliques.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from networkx import nx
G = nx.generators.ring_of_cliques(6, 3)
nx.draw(G, with_labels=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;gives you this:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-an-test-graph-with-known-max-cliques&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://rahulnair23.github.io/post/max-clique/graph_hu66fa372f34c55a7f200bf8e44ac04457_35825_2000x2000_fit_lanczos_3.png&#34; data-caption=&#34;An test graph with known max cliques&#34;&gt;


  &lt;img data-src=&#34;https://rahulnair23.github.io/post/max-clique/graph_hu66fa372f34c55a7f200bf8e44ac04457_35825_2000x2000_fit_lanczos_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    An test graph with known max cliques
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;step-1-generating-maximum-independent-sets&#34;&gt;(Step 1) Generating maximum independent sets&lt;/h4&gt;
&lt;p&gt;Here we use the greedy heuristic implementation as shown in this 
&lt;a href=&#34;https://kmutya.github.io/maxclique/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def greedy_init(G):
    &amp;quot;&amp;quot;&amp;quot;
    https://kmutya.github.io/maxclique/
    &amp;quot;&amp;quot;&amp;quot;
    n = G.number_of_nodes()  # Storing total number of nodes in &#39;n&#39;
    max_ind_sets = []  # initializing a list that will store maximum independent sets
    for j in G.nodes:
        R = G.copy()  # Storing a copy of the graph as a residual
        neigh = [n for n in R.neighbors(j)]  # Catch all the neighbours of j
        R.remove_node(j)  # removing the node we start from
        iset = [j]
        R.remove_nodes_from(neigh)  # Removing the neighbours of j
        if R.number_of_nodes() != 0:
            x = get_min_degree_vertex(R)
        while R.number_of_nodes() != 0:
            neigh2 = [m for m in R.neighbors(x)]
            R.remove_node(x)
            iset.append(x)
            R.remove_nodes_from(neigh2)
            if R.number_of_nodes() != 0:
                x = get_min_degree_vertex(R)

        max_ind_sets.append(iset)

    return(max_ind_sets)


def get_min_degree_vertex(Residual_graph):
    &#39;&#39;&#39;Takes in the residual graph R and returns the node with the lowest degree&#39;&#39;&#39;
    degrees = [val for (node, val) in Residual_graph.degree()]
    node = [node for (node, val) in Residual_graph.degree()]
    node_degree = dict(zip(node, degrees))
    return (min(node_degree, key=node_degree.get))
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;step-2-the-optimization-model&#34;&gt;(Step 2) The optimization model&lt;/h4&gt;
&lt;p&gt;Now we define the optimization model as a linear program using the CPLEX python API. We initialize the set of constraints based on the greedy heuristic.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cplex

prob = cplex.Cplex()
numvar = len(G.nodes)

def func(x): return &amp;quot;x&amp;quot;+str(x)

names = list(map(func, G.nodes))
var_type = [prob.variables.type.continuous] * numvar
prob.variables.add(names=names,
                   lb=[0.0] * numvar,
                   ub=[1.0] * numvar,
                   types=var_type)
prob.objective.set_sense(prob.objective.sense.maximize)
prob.objective.set_linear([(n, 1.0) for n in names])
lhs = []

# Call the greedy heuristic to generate a starting set of independent sets
mis = greedy_init(G)

for iset in mis:
    con_vars = [func(i) for i in iset]
    coeffs = [1.0] * len(con_vars)
    lhs.append(cplex.SparsePair(con_vars, coeffs))
prob.linear_constraints.add(lin_expr=lhs,
                            rhs=[1.0] * len(lhs),
                            senses=[&#39;L&#39;] * len(lhs))
print(&amp;quot;Constraint: Maximum independent set. ({} constraints)&amp;quot;.format(len(mis)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To solve this model, we would use the following snippet to execute the model and return the final solution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prob.solve()
status = prob.solution.status[prob.solution.get_status()]
print(&amp;quot;Status:{}&amp;quot;.format(status))

if prob.solution.get_status() in [101, 105, 107, 111, 113]:
    # Optimal/feasible, so get the solution
    print(&amp;quot;Solution value: &amp;quot;)
    print(prob.solution.get_objective_value())

    # get the configuration
    x_res = prob.solution.get_values(names)
    for x_name, val in zip(names, x_res):
        if val &amp;gt; 0:
            print(x_name, val)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;step-3-generate-lazy-constraints&#34;&gt;(Step 3) Generate lazy constraints&lt;/h4&gt;
&lt;p&gt;We&amp;rsquo;ve not managed the lazy constraints yet. To do this we will use a CPLEX &lt;code&gt;Callback&lt;/code&gt;. The &lt;code&gt;LazyConstraintCallback&lt;/code&gt; is called each time an optimal or integral solution is found. The implementation looks like this.&lt;/p&gt;
&lt;p&gt;To find new independent sets, we take the solution (potentially fractional) and use a greedy heuristic to first generate an independent set on the induced subgraph of the solution. We then expand on the independent set for the entire graph using another &lt;code&gt;greedy_expand&lt;/code&gt; procedure which uses the same logic as &lt;code&gt;greedy_init&lt;/code&gt; to grown the independent set.&lt;/p&gt;
&lt;p&gt;If there are no additional independent sets, no constraints are added and the solver terminates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class LazyCallback(LazyConstraintCallback):
    &amp;quot;&amp;quot;&amp;quot;Lazy constraint callback to generate maximum independent sets on the fly.

    There are too many such constraints to make them all available to 
    CPLEX right away - and in any case, very few of them are valid.

    So generate them on the fly.
    &amp;quot;&amp;quot;&amp;quot;

    # Callback constructor.
    #
    # Any needed fields are set externally after registering the callback.
    def __init__(self, env):
        super().__init__(env)

    def __call__(self):

        values = self.get_values(self.names)

        # Any node with non-zero value is considered as part of the set
        curr_solution = [int(name[1:]) for name, val in zip(self.names, values) if val &amp;gt;= 0.001]
        print(&amp;quot;Current solution: &amp;quot;, curr_solution)

        # Look to generate all independent sets that would cut off the (fractional)
        # value. To do this, first induce a subgraph - and for each node, built a
        #
        subG = self.G.subgraph(curr_solution)
        sub_ind_set = greedy_init(subG)
        max_ind_sets = [greedy_expand(self.G, sset) for sset in sub_ind_set]

        for iset in max_ind_sets:

            con_vars = [func(i) for i in iset]
            coeffs = [1.0] * len(con_vars)
            lhs = cplex.SparsePair(con_vars, coeffs)
            self.add(constraint=lhs, rhs=1.0, sense=&#39;L&#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The callback must be registered with the problem instance and any variables passed as attributes as so:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from cplex.callbacks import LazyConstraintCallback

# register callbacks to generate additional independent sets on the fly
lazycb = prob.register_callback(LazyCallback)

# pass any arguments as class attributes
lazycb.names = names
lazycb.G = G
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For completeness, here is what the &lt;code&gt;greedy_expand&lt;/code&gt; function does&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def greedy_expand(G, init_set):

    R = G.copy()
    neigh = [n for i in init_set for n in R.neighbors(i)]
    R.remove_nodes_from(init_set)
    R.remove_nodes_from(neigh)
    if R.number_of_nodes() != 0:
        x = get_min_degree_vertex(R)
    while R.number_of_nodes() != 0:
        neigh2 = [m for m in R.neighbors(x)]
        R.remove_node(x)
        init_set.append(x)
        R.remove_nodes_from(neigh2)
        if R.number_of_nodes() != 0:
            x = get_min_degree_vertex(R)

    return init_set
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;putting-it-all-together&#34;&gt;Putting it all together&lt;/h4&gt;
&lt;p&gt;Running this all together as shown in this 
&lt;a href=&#34;https://gist.github.com/rahulnair23/ef3c14a3f08afdf0840459e10969eda8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gist&lt;/a&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/rahulnair23/ef3c14a3f08afdf0840459e10969eda8.js&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;gives the following output:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Constraint: Maximum independent set. (18 constraints)
Version identifier: 12.10.0.0 | 2019-11-26 | 843d4de
CPXPARAM_Read_DataCheck                          1
Warning: Control callbacks may disable some MIP features.
Lazy constraint(s) or lazy constraint/branch callback is present.
    Disabling dual reductions (CPX_PARAM_REDUCE) in presolve.
    Disabling non-linear reductions (CPX_PARAM_PRELINEAR) in presolve.
    Disabling presolve reductions that prevent crushing forms.
         Disabling repeat represolve because of lazy constraint/incumbent callback.
Tried aggregator 1 time.
MIP Presolve eliminated 6 rows and 0 columns.
Reduced MIP has 12 rows, 18 columns, and 72 nonzeros.
Reduced MIP has 0 binaries, 0 generals, 0 SOSs, and 0 indicators.
Presolve time = 0.00 sec. (0.02 ticks)
MIP emphasis: balance optimality and feasibility.
MIP search method: traditional branch-and-cut.
Parallel mode: none, using 1 thread.
Root relaxation solution time = 0.00 sec. (0.01 ticks)
Current solution:  [5, 7, 8, 14, 16, 17]
Current solution:  [4, 10, 11, 13]
Current solution:  [0, 6, 8, 10, 11]
Current solution:  [4, 6, 8, 10, 11, 16]
Current solution:  [0, 2, 4, 5, 6, 8]
Current solution:  [6, 7, 8]
Current solution:  [6, 7, 8]

        Nodes                                         Cuts/
   Node  Left     Objective  IInf  Best Integer    Best Bound    ItCnt     Gap         Variable B NodeID Parent  Depth

*     0     0      integral     0        3.0000        6.0000        8  100.00%                        0             0
Elapsed time = 0.02 sec. (0.16 ticks, tree = 0.00 MB, solutions = 1)

User cuts applied:  17

Root node processing (before b&amp;amp;c):
  Real time             =    0.02 sec. (0.16 ticks)
Sequential b&amp;amp;c:
  Real time             =    0.00 sec. (0.00 ticks)
                          ------------
Total (root+branch&amp;amp;cut) =    0.02 sec. (0.16 ticks)
Status:MIP_optimal
Solution value: 
3.0
x6 1.0
x7 1.0
x8 1.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This identified one of the 3-vertex cliques, which is the maximum. The program started with 18 maximum independent sets generated greedily. It generated a further 17 user cuts one for each new independent set that were constructed on the fly. For a graph with $n$ nodes, there can be at most $3^{n/3}$ maximum independent sets although most have far fewer. For our 18 node graph, that would be 729 sets. In this greedy solution method, we got away with generating just 35.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
